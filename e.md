We present an effective post-processing method
that transforms and enhances contextual word representations through static anchors with guidance
from other contextualized/static embeddings. We
show leveraging static embeddings, with no labeled
data, consistently improves (across almost all configurations) on both Inter Word and surprisingly
Within Word context-aware lexical semantic tasks.
We also perform controlled analysis to highlight, in
isolation, the improvement from the transformation
on both contextualization and on an overall interword semantic space. In the future, we plan to apply the transformed representations on more lexical
semantics tasks such as word sense disambiguation
within an application (Navigli and Vannella, 2013)
