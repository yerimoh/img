5 Conclusion

In this paper, we propose to augment LLMs with long-term memory for enabling them to memorize
long-form context and gain long-form memory. The designed decoupled memory module can cache
attention key and value pairs of past inputs for future retrieval and fusion. A decoupled residual
SideNet is introduced as the memory retriever and reader, meanwhile the LLM itself is frozen
and works as knowledge and memory encoder. Experiments on various long-contextual language
modeling datasets demonstrate the effectiveness of our model over other memory-augmentation
baselines. The proposed method can also enable in-context learning of LLMs to overcome the limited
number of demonstration examples in context, which is constrained by the contextual length, via
caching thousands of auxiliary demonstration examples in memory.
