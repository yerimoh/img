Neural language representation models such
as BERT pre-trained on large-scale corpora
can well capture rich semantic patterns from
plain text, and be fine-tuned to consistently improve the performance of various NLP tasks.
However, the existing pre-trained language
models rarely consider incorporating knowledge graphs (KGs), which can provide rich
structured knowledge facts for better language
understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language
representation model (ERNIE), which can
take full advantage of lexical, syntactic, and
knowledge information simultaneously. The
experimental results have demonstrated that
ERNIE achieves significant improvements on
various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art
model BERT on other common NLP tasks.
The source code and experiment details of
this paper can be obtained from https://
github.com/thunlp/ERNIE.




1 Introduction
Pre-trained language representation models, including feature-based (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2017, 2018) and
fine-tuning (Dai and Le, 2015; Howard and Ruder,
2018; Radford et al., 2018; Devlin et al., 2019)
approaches, can capture rich language information from text and then benefit many NLP applications. BERT (Devlin et al., 2019), as one of the
most recently proposed models, obtains the stateof-the-art results on various NLP applications by
simple fine-tuning, including named entity recog
answering (Rajpurkar et al., 2016; Zellers et al.,
2018), natural language inference (Bowman et al.,
2015), and text classification (Wang et al., 2018).


Although pre-trained language representation
models have achieved promising results and
worked as a routine component in many NLP
tasks, they neglect to incorporate knowledge information for language understanding. As shown
in Figure 1, without knowing Blowin’ in the Wind
and Chronicles: Volume One are song and book
respectively, it is difficult to recognize the two occupations of Bob Dylan, i.e., songwriter and
writer, on the entity typing task. Furthermore,
it is nearly impossible to extract the fine-grained
relations, such as composer and author on
the relation classification task. For the existing
pre-trained language representation models, these
two sentences are syntactically ambiguous, like
“UNK wrote UNK in UNK”. Hence, considering
rich knowledge information can lead to better language understanding and accordingly benefits various knowledge-driven applications, e.g. entity
typing and relation classification.


For incorporating external knowledge into language representation models, there are two main
arXiv:1905.07129v3 [cs.CL] 4 Jun 2019
challenges. (1) Structured Knowledge Encoding: regarding to the given text, how to effectively
extract and encode its related informative facts in
KGs for language representation models is an important problem; (2) Heterogeneous Information
Fusion: the pre-training procedure for language
representation is quite different from the knowledge representation procedure, leading to two individual vector spaces. How to design a special
pre-training objective to fuse lexical, syntactic,
and knowledge information is another challenge.

To overcome the challenges mentioned above,
we propose Enhanced Language RepresentatioN
with Informative Entities (ERNIE), which pretrains a language representation model on both
large-scale textual corpora and KGs:


(1) For extracting and encoding knowledge information, we firstly recognize named entity mentions in text and then align these mentions to their
corresponding entities in KGs. Instead of directly
using the graph-based facts in KGs, we encode the
graph structure of KGs with knowledge embedding algorithms like TransE (Bordes et al., 2013),
and then take the informative entity embeddings
as input for ERNIE. Based on the alignments between text and KGs, ERNIE integrates entity representations in the knowledge module into the underlying layers of the semantic module.



(2) Similar to BERT, we adopt the masked language model and the next sentence prediction as
the pre-training objectives. Besides, for the better fusion of textual and knowledge features, we
design a new pre-training objective by randomly
masking some of the named entity alignments in
the input text and asking the model to select appropriate entities from KGs to complete the alignments. Unlike the existing pre-trained language
representation models only utilizing local context
to predict tokens, our objectives require models
to aggregate both context and knowledge facts for
predicting both tokens and entities, and lead to a
knowledgeable language representation model.



We conduct experiments on two knowledgedriven NLP tasks, i.e., entity typing and relation
classification. The experimental results show that
ERNIE significantly outperforms the state-of-theart model BERT on these knowledge-driven tasks,
by taking full advantage of lexical, syntactic, and
knowledge information. We also evaluate ERNIE
on other common NLP tasks, and ERNIE still
achieves comparable results.











