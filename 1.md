• We propose an LLM-based method to filter out texts for a
given scientific domain that enables fast data pre-processing
to build domain LLMs at a lower cost.
• We provide heuristics for the large batch training of LLMs,
which forms the basis for scaling up LLM training on HPC
systems.
• We introduce baseline analyses for communication requirements and establish the baseline scaling behavior for LLM
training on the first exascale system, Frontier.
• We release a set of open foundation models (and domain
datasets) on scientific corpora.
• We propose three tasks related to science to evaluate LLMs
for scientific applications.
