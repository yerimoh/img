Language models (LMs) such as GPT-3 (Brown
et al., 2020) and PaLM (Chowdhery et al., 2022)
have shown impressive abilities in a range of natural language processing (NLP) tasks. However, relying solely on their parameters to encode a wealth
of world knowledge requires a prohibitively large
number of parameters and hence massive compute,
and they often struggle to learn long-rail knowledge (Roberts et al., 2020; Kandpal et al., 2022;
Mallen et al., 2022). Moreover, these parametric LMs are fundamentally incapable of adapting
over time (De Cao et al., 2021; Lazaridou et al.,
2021; Kasai et al., 2022), often hallucinate (Shuster et al., 2021), and may leak private data from the
training corpus (Carlini et al., 2021). To overcome
these limitations, there has been growing interest
in retrieval-based LMs (Guu et al., 2020; Khandelwal et al., 2020; Borgeaud et al., 2022; Zhong
et al., 2022; Izacard et al., 2022b; Min et al., 2022),
which incorporate a non-parametric datastore (e.g.,
text chunks from an external corpus) with their
parametric counterparts. Retrieval-based LMs can
outperform LMs without retrieval by a large margin with much fewer parameters (Mallen et al.,
2022), can update their knowledge by replacing
their retrieval corpora (Izacard et al., 2022b), and
provide citations for users to easily verify and evaluate the predictions (Menick et al., 2022; Bohnet
et al., 2022).



Previously, retrieval and LMs have been studied
mostly separately, and only recently researchers
have integrated them and built systems in which
retrieval and LMs interact more organically, and a
number of retrieval-based LMs have been proposed
due to growing interest. They differ in their neural
architectures (e.g., the granularity of retrieval units,
how to integrate retrieved information), learning
algorithms, and different uses in downstream applications. In this tutorial, we aim to provide a
comprehensive and coherent overview of recent
advances in retrieval-based LMs. We will start
by first providing preliminaries covering the foundations of LM (e.g., masked LMs, autoregressive
LMs) and retrieval systems (e.g., nearest-neighbor
search methods widely used in neural retrieval systems; Karpukhin et al. 2020). We will then focus
on recent progress in architectures, learning approaches, and applications of retrieval-based LMs






A taxonomy of architectures 


We introduce a
taxonomy of architectures of retrieval-based LMs
based on a variety of dimensions. Retrieval-based
LMs can be categorized by the granularity of retrieved units stored in the datastore: either 1) a
chunk of text (Borgeaud et al., 2022; Izacard et al.,
2022b), or 2) a token (Khandelwal et al., 2020;
Zhong et al., 2022; Min et al., 2022), or 3) an entity mention (Févry et al., 2020; de Jong et al.,
2022). We also plan to cover techniques for refining data stores and improving similarity search (He
et al., 2021; Alon et al., 2022). At the same time,
retrieval-base LMs can be categorized based on
how the retrieved information is integrated with
the parametric encoder: 1) whether retrieved components are concatenated with the original input
text (Lewis et al., 2020; Guu et al., 2020; Izacard
et al., 2022b), 2) whether the retrieved components
are latent and integrated into the intermediate layers of Transformers (de Jong et al., 2022; Févry
et al., 2020; Borgeaud et al., 2022), or 3) distribution of tokens from the retrieved components and
the LMs are interpolated (Khandelwal et al., 2020;
Zhong et al., 2022; Yogatama et al., 2021).




Scalable learning algorithms 

Then, we discuss
the training approaches of retrieval-based LMs.
Since a retrieval datastore is typically very large,
how to train retrieval-based LMs effectively and
efficiently remains challenging. We first discuss
pipelined approaches that train retrieval components and LMs separately, either through large
scale pre-training (Izacard et al., 2022a) or multitask instruction tuning (Asai et al., 2022). Several
other works train retrieval-based LMs with a fixed
retrieval module (Borgeaud et al., 2022; Yogatama
et al., 2021). We then discuss joint training under
reasonable resource requirements: either through
in-batch approximations to a full datastore, or updating the datastore with updated parameters asynchronously. The former uses fractions of the full
corpus that are carefully designed during joint training (Zhong et al., 2022; de Jong et al., 2022; Min
et al., 2022). The latter, on the other hand, aims to
use full corpus during training with asynchronous
index update for every certain time steps (Izacard
et al., 2022b; Guu et al., 2020).









