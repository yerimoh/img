We match a MatSci DB of materials and compositions with tables from published papers, to automatically provide distantly-supervised labels for
extraction. We first use a commercial DB (NGF,
2019) of glass compositions with the respective
references. Then, we extract all tables from the
2,536 references in the DB using text-mining API
(els). We use a table parser (Jensen et al., 2019a)
for raw XML tables and captions. This results in
5,883 tables of which 2,355 express compositions
with 16,729 materials, and 58,481 (material ID,
constituent, composition percentage, unit) tuples.
We keep tables from 1,880 papers for training, and
the rest are split into dev and test (see Table 4b).


The DB does not contain information about the
location of a given composition in the paper – in
text, images, graphs, or tables. If present in a table, it can appear in any column or row. Since we
do not know the exact location of a composition,
we use distantly supervised train set construction
(Mintz et al., 2009). First, we simply match the
chemical compounds and percentages (or equivalent fractions) mentioned in the DB with the text in
a table from the associated paper. If all composition percentages are found in multiple cells of the
table, it is marked as MCC-CI (multi-cell composition with complete information). However, due to
several problems (see Appendix 3), it misses many
composition tables. To increase the coverage, we
additionally use a rule-based composition parser
(described below), but restricted to only those compounds (CPD non-terminal in Figure 2) that appear
in the DB for this paper.


Our distant supervision approach obtains tablelevel annotation (NC, SCC, MCC-PI, MCC-CI),
where a table is labeled as non-composition, single/multi cell composition with partial/complete
information. It also obtains annotation for each
row or column into four labels: ID, composition,
constituent, and other. While training data is created using distant supervision, dev and test sets
are hand annotated. We now explain the dataset
construction process in further detail.




Rule-based composition parser: The parser
helps find names of constituents from MCC tables, and also match full compositions mentioned
in SCC tables. Recall that in SCC tables, the full
composition expression is written in a single cell in
the row/column corresponding to each Material ID.
Such compositions are close to regular languages
and can be parsed via regular expressions



Figure 2
shows the regular expression
(simplified, for
understandability) used
by the parser.
Here CMP
denotes the matched composition, PATs are the
three main patterns for it, CSTs are sub-patterns,
CPD is a compound, NUM is a number, and
OB and CB are, respectively, open and closed
parentheses (or square brackets). W is zero or
more whitespace characters, and SEP contains
explicit separators like ‘-’ or ‘+’. START and END
are indicators to separate a regular expression from
the rest of the text.




The first pattern parses simple numbercompound expressions like 40Bi2O3 * 60B2O3.
Here each of the two constituents will match with
CST1. The other two patterns handle nested compositions, where simple expressions are mixed in
a given ratio. The main difference between the
second and third patterns is in the placement of
outer ratios – after or before the simple composition, respectively. Example match for PAT2 is
(40Bi2O3+60B2O3)30 - (AgI+AgCl)70, and for
PAT3 is 40Bi2O3,40B2O3,20(AgI:2AgCl).





To materialize the rules of the rule-based composition parser, we pre-label compounds. For our
dataset, we use a list-based extractor, though other
chemical data extractors (Swain and Cole, 2016b)
may also be used. After parsing, all coefficients are
normalized so that they sum to hundred. For nested
expressions, the outer ratio and the inner ones are
normalized separately and then multiplied.
The compositions parsed by rule-based composition parser are then matched with entries in the DB.
A successful matching leads to a high-quality annotation of composition expressions in these papers.
If this matching happens: (i) in a single cell, the
table is deemed as SCC, (ii) on caption/paper text
that has an algebraic variable (or compound) found
in the table, it is marked as MCC-PI (see Figure
1(b)). In case of no matching, the table is marked
as NC. This automatic annotation is post-processed
into row, column and edge labels.



One further challenge is that material IDs mentioned in papers are not provided in the DB. So, we
manually annotate material IDs for all the identified
composition tables in the training set. This leads to
a train set of 11,207 materials with 38,799 tuples
from 4,408 tables. Since the train set is distantly supervised and can be noisy, two authors (one of them
is a MatSci expert) of this paper manually annotated the dev and test tables with row/column/edge
labels, units, tuples, compositions, and table type,
resulting in over 2,500 materials and over 9,500
tuples per set. We used Cohen’s Kappa measure for
identifying inter-annotator agreement, which was
86.76% for Glass ID, 98.47% for row and column
labels, and 94.34% for table types. Conflicts were
resolved through mutual discussions. Further statistics and the description of the developed in-house
annotation tools used for manual annotations are
discussed in A.2.








