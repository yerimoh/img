
Figure 1: Exemplary predictions of Toolformer. The
model autonomously decides to call different APIs
(from top to bottom: a question answering system,
a calculator, a machine translation system, and a
Wikipedia search engine) to obtain information that is
useful for completing a piece of text.





Our aim is to equip a language model M with the
ability to use different tools by means of API calls.
We require that inputs and outputs for each API
can be represented as text sequences. This allows
seamless insertion of API calls into any given text,
using special tokens to mark the start and end of
each such call.

We represent each API call as a tuple c = (ac, ic)
where ac is the name of the API and ic is the corresponding input. Given an API call c with a corresponding result r, we denote the linearized sequences of the API call not including and including
its result, respectively, as:


where “<API>”, “</API>” and “→” are special
tokens.1 Some examples of linearized API calls
inserted into text sequences are shown in Figure 1.Given a dataset C = {x
1
, . . . , x
|C|} of plain
texts, we first convert this dataset into a dataset
C
∗
augmented with API calls. This is done in three
steps, illustrated in Figure 2: First, we exploit the
in-context learning ability of M to sample a large
number of potential API calls. We then execute
these API calls and finally check whether the obtained responses are helpful for predicting future
tokens; this is used as a filtering criterion. After
filtering, we merge API calls for different tools,
resulting in the augmented dataset C
∗
, and finetune M itself on this dataset. Each of these steps is
described in more detail below.


