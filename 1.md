The input to the CNN for relation extraction consists
of sentences marked with the two entity mentions of
interest. As CNNs can only work with fixed length
inputs, we compute the maximal separation between
entity mentions linked by a relation and choose an
input width greater than this distance. We insure
that every input (relation mention) has this length
by trimming longer sentences and padding shorter
sentences with a special token.
Let n be the length of the relation mentions and
x = [x1, x2, . . . , xn] be some relation mention
where xi
is the i-th word in the mention. Also, let
xi1
and xi2
be the two heads of the two entity mentions of interest . Before entering the network, each
word xi
is first transformed into a vector ei by looking up the word embedding table W that can be initialized either by a random process or by some pretrained word embeddings. Besides, in order to embed the positions of the two entity heads as well as
the other words in the relation mention into the representation, for each word xi
, its relative distances to
the two entity heads i−i1 and i−i2 are also mapped
into real-value vectors di1
and di2
respectively using
a position embedding table D (initialized randomly)
(Collobert et al., 2011; Liu et al., 2013; Zeng et al.,
2014). Note that the relative distances only range
from −n + 1 to n − 1 so the position embedding
matrix D has size (2n − 1) × md (md is a hyperparameter indicating the dimensionality of the position
embedding vectors). Finally, the word embeddings
