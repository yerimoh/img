Offensive content is an unavoidable issue on
social media. Most existing offensive language
identification methods rely on the compilation
of labeled datasets. However, existing methods rarely consider low-resource languages that
have relatively less data available for training
(e.g., Korean). To address these issues, we construct a novel KOrean Dataset for Offensive
Language Identification (KODOLI). KODOLI
comprises more fine-grained offensiveness categories (i.e., not offensive, likely offensive, and
offensive) than existing ones. A likely offensive language refers to texts with implicit offensiveness or abusive language without offensive intentions. In addition, we propose two
auxiliary tasks to help identify offensive languages: abusive language detection and sentiment analysis. We provide experimental results for baselines on KODOLI and observe that
pre-trained language models suffer from identifying "LIKELY" offensive statements. Quantitative results and qualitative analysis demonstrate that jointly learning offensive language,
abusive language and sentiment information improves the performance of offensive language
identification
