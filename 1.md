Advanced knowledge of a science or engineering
domain is typically found in domain-specific research papers. Information extraction (IE) from
scientific articles develops ML methods to automatically extract this knowledge for curating largescale domain-specific KBs (e.g., (Ernst et al., 2015;
Hope et al., 2021)). These KBs have a variety of
uses: they lead to ease of information access by domain researchers (Tsatsaronis et al., 2015; Hamon
et al., 2017), provide data for developing domainspecific ML models (Nadkarni et al., 2021), and
potentially help in accelerating scientific discoveries (Jain et al., 2013; Venugopal et al., 2021).


Significant research exists on IE from text of research papers (see Nasar et al. (2018) for a survey),
but less attention is given to IE (often, numeric)
from tables. Tables may report the performance
of algorithms on a dataset, quantitative results of
clinical trials, or other important information. Of
special interest to us are tables that mention the
composition and properties of an entity. Such tables are ubiquitous in various fields such as food
and nutrition (tables of food items with nutritional
values, see Tables 1-4 in de Holanda Cavalcanti
et al. (2021) and Table 2 in Stokvis et al. (2021)),
fuels (constituents and calorific values, see Table
2 in Kar et al. (2022) and Beliavskii et al. (2022)),
building construction (components and costs, see
Table 4 in Aggarwal and Saha (2022)), materials (constituents and properties, see Table 1 and
2 in Kasimuthumaniyan et al. (2020) and Table
4 in Keshri et al. (2022)), medicine (compounds
with weights in drugs, see Table 1 in Kalegari et al.
(2014)), and more.





In materials science (MatSci) articles, the details
on synthesis and characterization are reported in
the text (Mysore et al., 2019), while material compositions are mostly reported in tables (Jensen et al.,
2019b). A preliminary analysis of MatSci papers
reveals that ∼85%1 of material compositions and
their associated properties (e.g., density, stiffness)
are reported in tables and not text. Thus, IE from tables is essential for a comprehensive understanding
of a given paper, and for increasing the coverage
of resulting KBs. To this extent, we define a novel
NLP task of extraction of materials (via IDs mentioned in the paper), constituents, and their relative
percentages. For instance, Fig.1a should output
four materials A1-A4, where ID A1 is associated
with three constituents (MoO3, Fe2O3, and P2O5)
and their respective percentages, 5, 38, and 57.
A model for this task necessitates solving several
challenges, which are discussed in detail in Sec. 3.
While many of these issues have been investigated
separately, e.g., numerical IE (Madaan et al., 2016),
unit extraction (Sarawagi and Chakrabarti, 2014),
chemical compound identification (Weston et al.,
2019), NLP for tables (Jensen et al., 2019b; Swain
and Cole, 2016a), solving all these in concert creates a challenging testbed for the NLP community





Here, we harvest a distantly supervised training
dataset of 4,408 tables and 38,799 compositionconstituent tuples by aligning a MatSci database
with tables in papers. We also label 1,475 tables
manually for dev and test sets. We build a baseline system DISCOMAT, which uses a pipeline of
a domain-specific language model (Gupta et al.,
2022), and two graph neural networks (GNNs),
along with several hand-coded features and constraints. We evaluate our system on accuracy metrics for various subtasks, including material ID prediction, tuple-level predictions, and material-level
complete predictions. We find that DISCOMAT’s
GNN architecture obtains a 7-15 points increase in
accuracy numbers, compared to table processors
(Herzig et al., 2020; Yin et al., 2020), which linearize the table for IE. Subsequent analysis reveals
common sources of DISCOMAT errors, which will
inform future research. We release all our data and
code2
for further research on this challenging task.



















