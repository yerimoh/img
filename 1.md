• The use of tools should be learned in a
self-supervised way without requiring large
amounts of human annotations. This is important not only because of the costs associated
with such annotations, but also because what
humans find useful may be different from
what a model finds useful.

• The LM should not lose any of its generality
and should be able to decide for itself when
and how to use which tool. In contrast to
existing approaches, this enables a much more
comprehensive use of tools that is not tied to
specific tasks.



Our approach for achieving these goals is based
on the recent idea of using large LMs with incontext learning (Brown et al., 2020) to generate
entire datasets from scratch (Schick and Schütze,
2021b; Honovich et al., 2022; Wang et al., 2022):
Given just a handful of human-written examples
of how an API can be used, we let a LM annotate
a huge language modeling dataset with potential
API calls. We then use a self-supervised loss to
determine which of these API calls actually help
the model in predicting future tokens. Finally, we
finetune the LM itself on the API calls that it considers useful. As illustrated in Figure 1, through
this simple approach, LMs can learn to control a variety of tools, and to choose for themselves which
tool to use when and how.

As our approach is agnostic of the dataset being used, we can apply it to the exact same dataset
that was used to pretrain a model in the first place.
This ensures that the model does not lose any
of its generality and language modeling abilities.
We conduct experiments on a variety of different downstream tasks, demonstrating that after
learning to use tools, Toolformer, which is based
on a pretrained GPT-J model (Wang and Komatsuzaki, 2021) with 6.7B parameters, achieves much
stronger zero-shot results, clearly outperforming a
much larger GPT-3 model (Brown et al., 2020) and
several other baselines on various tasks.
