We present BTR, a fast and storage-efficient reader for retrieval-augmented language models. BTR
creates cacheable calibrated binary representations for the retrieved passages to speed up inference
and avoid large storage overhead. We describe BTR for encoder-decoder readers which are most
widely used in the state-of-the-art models (Izacard et al., 2022b; Izacard & Grave, 2021), and later
show how to apply BTR for the encoder-only model (ยง4). The encoder and encoder-decoder readers
both need to process many passages, which is the computation bottleneck. Previous solutions (Cao
et al., 2020; Lee et al., 2021) precompute the passage representations to avoid runtime computation
but come with large amounts of storage costs (e.g. terabytes of storage for Wikipedia scale corpus).
BTR tackles the challenge by building compact binary token representations for the passages.



Overview. 

Figure 2 summarizes the BTR reader architecture and how our techniques are applied to
each component. The key idea in BTR is to create cacheable binary token representations (ยง3.1)
of the retrieved passages such that the passage encoding can be precomputed offline and stored in a
compact format. We build on the previous technique (Cao et al., 2020) to decompose the passage
and query computation in the lower layers of the reader encoder and jointly process the query and
passage representations back in the upper layers. However, unlike them, we create calibrated binary
passage representations after the decomposition to drastically reduce storage. We further develop
offline compression to reduce the storage of these precomputed binary token representations. Such
binarization and decomposition incur task performance degradation, which we address by designing
two regularization techniques during training (ยง3.2). For inference, we develop runtime compression
techniques (ยง3.3) to further speed up the reader computation
