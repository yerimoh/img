Abstract

Language models (LMs) exhibit remarkable
abilities to solve new tasks from just a few
examples or textual instructions, especially at
scale. They also, paradoxically, struggle with
basic functionality, such as arithmetic or factual lookup, where much simpler and smaller
models excel. In this paper, we show that
LMs can teach themselves to use external tools
via simple APIs and achieve the best of both
worlds. We introduce Toolformer, a model
trained to decide which APIs to call, when to
call them, what arguments to pass, and how to
best incorporate the results into future token
prediction. This is done in a self-supervised
way, requiring nothing more than a handful of
demonstrations for each API. We incorporate
a range of tools, including a calculator, a Q&A
system, a search engine, a translation system,
and a calendar. Toolformer achieves substantially improved zero-shot performance across
a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.


1 Introduction

Large language models achieve impressive zeroand few-shot results on a variety of natural language processing tasks (Brown et al., 2020; Chowdhery et al., 2022, i.a.) and show several emergent
capabilities (Wei et al., 2022). However, all of
these models have several inherent limitations that
can at best be partially addressed by further scaling. These limitations include an inability to access
up-to-date information on recent events (Komeili
et al., 2022) and the related tendency to hallucinate
facts (Maynez et al., 2020; Ji et al., 2022), difficulties in understanding low-resource languages (Lin
et al., 2021), a lack of mathematical skills to perform precise calculations (Patel et al., 2021) and an
unawareness of the progression of time (Dhingra
et al., 2022)

A simple way to overcome these limitations of
todayâ€™s language models is to give them the ability to use external tools such as search engines,
calculators, or calendars. However, existing approaches either rely on large amounts of human
annotations (Komeili et al., 2022; Thoppilan et al.,
2022) or limit tool use to task-specific settings only
(e.g., Gao et al., 2022; Parisi et al., 2022), hindering a more widespread adoption of tool use in LMs.
Therefore, we propose Toolformer, a model that
learns to use tools in a novel way, which fulfills the
following desiderata:
