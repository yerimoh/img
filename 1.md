Recent advances in language models (LMs)
have dramatically increased the usefulness of
machine-generated text across a wide range of
use-cases and domains (Brown et al., 2020). However, the mainstream paradigm of generating text
with LMs bears inherent limitations in access
to external knowledge. First, LMs are not coupled with any source attribution, and must be
trained in order to incorporate up-to-date information that was not seen during training. More
importantly, they tend to produce factual inaccuracies and errors (Lin et al., 2022; Maynez
et al., 2020; Huang et al., 2020). This problem is
present in any LM generation scenario, and is exacerbated when generation is made in uncommon
domains or private data. A promising approach
for addressing the above is Retrieval-Augmented
Language Modeling (RALM), grounding the LM
during generation by conditioning on relevant
documents retrieved from an external knowledge
source. RALM systems include two high level
components: (i) document selection, selecting the
set of documents upon which to condition; and
(ii) document reading, determining how to incorporate the selected documents into the LM
generation process.



Leading RALM systems introduced recently
tend to be focused on altering the language model
architecture (Khandelwal et al., 2020; Borgeaud
et al., 2022; Zhong et al., 2022; Levine et al.,
2022c; Li et al., 2022). Notably, Borgeaud et al.
(2022) introduced RETRO, featuring document
reading via nontrivial modifications that require
further training to the LM architecture, while using an off-the-shelf frozen BERT retriever for
document selection. Although the paper’s experimental findings showed impressive performance
gains, the need for changes in architecture and dedicated retraining has hindered the wide adoption
of such models.




In this paper, we show that a very simple document reading mechanism can have a large impact,
and that substantial gains can also be made by
adapting the document selection mechanism to the
task of language modeling. Thus, we show that
many of the benefits of RALM can be achieved
while working with off-the-shelf LMs, even via
API access. Specifically, we consider a simple but
powerful RALM framework, dubbed In-Context
RALM (presented in Section 3), which employs
a zero-effort document reading mechanism: We
simply prepend the selected documents to the
LM’s input text (Figure 1).


























