2 Key Findings
We successfully reproduce and pretrain RETRO
(Borgeaud et al., 2022) from scratch1
, with parameter sizes ranging from 148M up to 9.5B by retrieving from a text corpus with over 330B tokens.
In addition, we discuss the inference strategy of
RETRO for text generation that is not covered in
Borgeaud et al. (2022), and perform a large-scale
evaluation in different scenarios.


To minimize the discrepancy variables between
RETRO and GPT, we use the same decoder architecture, same hyper-parameters, and same pre-training
corpus to pre-train RETRO and GPT given the same
number of pre-training steps. We highlight our
novel findings for RETRO and GPT as follows:


2.1 Text Generation

We conduct a systematic study (see §5) to understand and analyze RETRO by evaluating its openended text generation quality via human and automatic evaluations. RETRO exhibits better performance than GPT with considerably less repetition,
moderately higher factual accuracy, and slightly
lower toxicity levels. RETRO is on par with GPT in
terms of fluency, coherence



2.2 LM Evaluation Harness Benchmark

In terms of zero-shot evaluation on the standard
benchmark, RETRO can overall improve upon the
GPT across different tasks, significantly outperforming GPT on knowledge-intensive tasks such as
Hellaswag and BoolQ while achieving similar performance on other tasks. Specifically, we evaluate
the zero-shot capabilities of RETRO and GPT on
nine representative NLP downstream classification
tasks (see §6). Additionally, our findings demonstrate that RETRO can leverage retrieved neighbors
and significantly improves accuracy for knowledgeintensive tasks in zero-shot evaluations. In contrast,
incorporating these retrieved neighbors directly during the inference stage can hurt GPT’s performance.
These results further substantiate the potential of
RETRO, which is pre-trained with retrieval capabilities, as a promising approach.




2.3 Open-domain QA

For open-domain QA tasks, RETRO achieves
considerably superior performance than retrievalaugmented GPT that incorporates retrieval during fine-tuning across different model sizes and
datasets. Specifically, we propose a variant of the
model, RETRO++, for open-domain QA that feeds
the most relevant evidence into the decoder and
more evidence into its encoder, which is different
from the original version (Borgeaud et al., 2022).
RETRO++ can largely improve the exact matching
score (EM) on Natrual Question from 40.9% to
54.1%, which is significant higher than the 45.5%
reported by the original RETRO



