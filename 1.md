In this section, we present the experimental results
for detecting experiment-describing sentences, entity mention extraction and experiment slot identification. For tokenization, we employ ChemDataExtractor,8 which is optimized for dealing with chemical formulas and unit mentions.
We tune our models in a 5-fold cross-validation
setting. We also report the mean and standard deviation across those folds as development results.
For the test set, we report the macro-average of
the scores obtained when applying each of the
five models to the test set. To put model performance in relation to human agreement, we report
the corresponding statistics obtained from our interannotator agreement study (Section 5). Note that
these numbers are based on a subset of the data and
are hence not directly comparable.
Hyperparameters and training. The BiLSTM
models are trained with the Adam optimizer
(Kingma and Ba, 2015) with a learning rate of
1e-3. For fine-tuning the original BERT models,
we follow the configuration published by Wolf et al.
(2019) and use AdamW (Loshchilov and Hutter,
2019) as optimizer and a learning rate of 4e-7 for
sentence classification and 1e-5 for sequence tagging. When adding BERT tokens to the BiLSTM,
we also use the AdamW optimizer for the whole
model and learning rates of 4e-7 or 1e-5 for the
BERT part and 1e-3 for the remainder. For regularization, we employ early stopping on the development set. We use a stacked BiLSTM with two
hidden layers and 500 hidden units for all tasks
with the exception of the experiment sentence de
tection task, where we found one BiLSTM layer
to work best. The attention layer of the sentence
detection model has a hidden size of 100.




Experiment detection. The task of experiment
detection can be modeled as a binary sentence classification problem. It can also be conceived as
a retrieval task, selecting sentences as candidates
for experiment frame extraction. We implement a
bidirectional long short-term memory (BiLSTM)
1260
model with attention for the task of experiment sentence detection. Each input token is represented by
a concatenation of several pretrained word embeddings, each of which is fine-tuned during training.
We use the Google News word2vec embeddings
(Mikolov et al., 2013), domain-specific word2vec
embeddings (mat2vec, Tshitoyan et al., 2019, see
also Section 2), subword embeddings based on
byte-pair encoding (bpe, Heinzerling and Strube,
2018), BERT (Devlin et al., 2019), and SciBERT
(Beltagy et al., 2019) embeddings. For BERT and
SciBERT, we take the embeddings of the first word
piece as token representation. The embeddings
are fed into a BiLSTM model followed by an attention layer that computes a vector for the whole
sentence. Finally, a softmax layer decides whether
the sentence contains an experiment.
In addition, we fine-tune the original (uncased)
BERT (Devlin et al., 2019) as well as SciBERT
(Beltagy et al., 2019) models on our dataset. SciBERT was trained on a large corpus of scientific
text. We use the implementation of the BERT sentence classifier by Wolf et al. (2019) that uses the
CLS token of BERT as input to the classification
layer.5
Finally, we compare the neural network models with traditional classification models, namely
a support vector machine (SVM) and a logistic regression classifier. For both models, we use the following set of input features: bag-of-words vectors
indicating which 1- to 4-grams and part-of-speech
tags occur in the sentence.6



Experiments: identifying experimentdescribing sentences. P, R and F1 for experimentdescribing sentences. With the exception of SVM, we
downsample the non-experiment-describing sentences
of the training set by 0.3.



