Pre-trained language models (LMs) like BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019)
have significantly advanced NLP. Recently, LMs have also been used by many continual learning
(CL) systems to learn a sequence of end-tasks incrementally (Ke et al., 2021a; Sun et al., 2020;
Huang et al., 2021), which we call continual end-task learning. It is also desirable to continually
pre-train LMs themselves. This includes (1) continual general pre-training, which incrementally
updates the LM using the most recent data that has a similar distribution as the pre-training data, and
(2) continual domain-adaptive pre-training, which further pre-trains a LM incrementally to adapt it
to a sequence of domains. Note that LM editing (with or without continual learning) (Mitchell et al.,
2022) that corrects mistakes learned in the LM is a special case of continual end-task learning (Kim
et al., 2022) as each editing task or a group of editing tasks learned together is basically a task in
continual learning, which aims to perform the editings correctly without interfering with or forgetting
the other knowledge already learned in the current LM.




This paper focuses on continual domain-adaptive pre-training (or continual DAP-training) of LMs.
It is known that DAP-training2
an LM (without continual learning) using a large unlabeled domain
corpus before end-task fine-tuning achieves better results (Gururangan et al., 2020; Xu et al., 2019;
Ke et al., 2022b). This paper goes a step further to continually learn to improve an LMâ€™s ability to
handle new or emerging domains or topics without forgetting the skills or knowledge learned in the
past. This is important in the real world, where the data shifts constantly and new domains, events or
topics keep emerging (Ke et al., 2022b) and the LM needs to be updated to serve the users better.


We call this problem continual DAP-training. Starting from a pre-trained general LM (i.e., the LM
has already been pre-trained on D0), we incrementally DAP-train a sequence of domain corpora
D1, D2, .... Once a domain is trained, its data is no longer accessible. This is different from
conventional continual learning (CL) where each task is an end-task. In the proposed continual
DAP-training, each task is an unlabeled domain corpus to be learned. An end-task fine-tunes the
continually DAP-trained LM to evaluate its performance. It is worth noting that D0 is usually a
broad or general domain (e.g., News). In practice, a continually DAP-trained LM may be trained by
individual users, institutions or a mix of both who have one or more large corpora of some particular
domains. In such cases, the raw data may not be shared, but the final LM can be shared by all.



There are multiple desiderata for a continual DAP-training system: (1) It should not suffer from
catastrophic forgetting (CF), i.e., it should perform reasonably well on learned domains. This requires
the system (a) to overcome CF for each new domain and (b) to overcome CF for the general language
knowledge in the LM. This is important because the knowledge learned from each domain alone will
not be sufficient for good end-task performances. (2) It should encourage knowledge transfer (KT)
across domains to achieve improved end-task performances. This requires the system to enable (a)
forward transfer, learning a new domain by leveraging the knowledge from previous domains, and
(b) backwards transfer, gaining improved performance on previous domains after learning a relevant
new domain. (3) It should work without requiring the domain-ID for each end-task fine-tuning





None of the existing CL methods can achieve all the above. This paper represents a step towards
achieving them. The proposed method is called DAS (Continual DA-pre-training of LMs with
Soft-masking). DAS proposes a novel soft-masking mechanism that computes the importance (a real
number between 0 and 1) of units3
for general or domain knowledge and soft-mask them based on
their importance values to control the backward gradient flow. In the forward pass, soft-masking
is not applied, which encourages KT across domains. It does not isolate any sub-network for any
domain so that the knowledge in the full LM can be leveraged for end-task fine-tuning.




To apply this mechanism, DAS implements two functions: (1) Initialization, which computes the
importance of units to the general knowledge in the LM without accessing the LM pre-training
data (D0). It is applied on the pre-trained LM before the continual learning starts, and (2) continual
learning, which DAP-trains each domain while preventing CF on the general and domain knowledge
and encouraging cross-domain KT. In (1), it is not obvious how to compute the importance without
pre-training data. DAS proposes a novel proxy based on robustness to compute the importance of
units for the general knowledge. In (2), the soft-masking is directly applicable because we have
the domain data and the importance can be computed based on its gradient inspired by the pruning
community (Li et al., 2021; Michel et al., 2019). Moreover, DAS contrasts the previously learned
knowledge and the full (including both the learned domains and the current domain) knowledge to
encourage the current domain representation to learn knowledge that is not already in the knowledge
learned from previous domains and integrate it with the learned knowledge4
. In end-task fine-tuning,
DAS does not requires the domain-ID as all knowledge is accumulated into the DAP-trained LM.




In summary, this work makes the following contributions. (i) It studies the new problem of continual
DAP-training and discovers that the full LM is needed for a good continual DAP-training method.
The popular parameter-isolation approach to overcoming CF in convention CL is unsuitable. (ii)
It proposes a novel soft-masking method to overcome CF and to encourage KT, and a constrative
learning based method for knowledge integration. (iii) To preserve the general knowledge in the LM,
a novel proxy is also proposed. (iv) Experimental results demonstrate the effectiveness of DAS.




















