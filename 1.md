8 Discussion



Retrieval from external sources has become a common practice in knowledge-intensive tasks (such
as factual question answering, fact checking, and
more; Petroni et al. 2021). In parallel, recent breakthroughs in LM generation capabilities has led to
LMs that can generate useful long texts. However, factual inaccuracies remain a common way
in which machine-generated text can fall short,
and lack of direct provenance makes it hard to
trust machine generated text. This makes language
modeling both a promising and an urgent new
application area for knowledge grounding, and
motivates promoting RALM approaches. Prior research has already investigated RALM, of course,
but it is not yet widely deployed. One likely reason
is that existing approaches rely upon fine-tuning
the LM, which is typically difficult and costly,
and is even impossible for LMs accessible only
via an API.


This paper presented the framework of
In-Context RALM, enabling frozen, off-the-shelf
LMs to benefit from retrieval. We demonstrated
that substantial performance gains can be
achieved by using general purpose retrievers, and
showed that additional gains can be achieved
by tailoring the document selection to the LM
setting. A recent work by Muhlgay et al. (2023)
demonstrates that In-Context RALM is indeed
able to improve the factuality of large LMs.



Several directions for further improvement remain for future work. First, this paper considers
only the case of prepending a single external document to the context; adding more documents
could drive further gains (for example, using the
framework of Ratner et al., 2022). Second, we retrieved documents every fixed interval of s tokens,
but see potential for large latency and cost gains
by retrieving more sparsely, such as only when a
specialized model predicts that retrieval is needed.



We release the code used in this work, for the
community to use and improve over. We hope it
will drive further research of RALM, which will
enable its wider adoption



