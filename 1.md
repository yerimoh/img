Most neural language models (LMs) process text generation tasks by making a series of next-token
predictions in an autoregressive manner (Radford et al., 2019; Dai et al., 2019; Khandelwal et al.,
2020; Shi et al., 2022). Specifically, LMs generate the next-token distribution over a fixed vocabulary
for any given prefix. Then, the next token is selected by a chosen decoding method, such as greedy
search and nucleus sampling (Holtzman et al., 2020). This process continues until some stop condition
is reached. For example, a special end-of-generation token is emitted, or the generated text reaches
the maximum length limit.

Unlike traditional neural language models, we reformulate text generation by copying text segments
from existing text collections. The text segments can be of variable lengths, including single words
and multi-word phrases. For clarity, we will use the term “phrase” to refer to any contiguous text
segments, and a single word can also be seen as a phrase of length 1. We compute a contextualized
vector representation for each phrase and pack them into an offline index. At each decoding step,
a suitable phrase is retrieved from the offline index and appended to the current prefix. In other
words, the next-token predictions in traditional neural language models are replaced by a series of
copy-and-paste operations.


Our proposed model, named COG (short for COPY-GENERATOR), enjoys the following advantages.
First, our method selects phrases in specific contexts rather than standalone tokens in a fixed vocabulary. It potentially allows for more accurate candidate representation and selection. Second, our
method allows training-free adaptation to new knowledge sources because the text collection can be
updated in a plug-and-play fashion. It could benefit application scenarios such as domain adaptation
and data expansion/filtering. Third, our method allows a sequence of multiple tokens (i.e., multi-wordWe conduct extensive experiments to verify the effectiveness of our proposed COG. On the standard
language modeling benchmark (WikiText-103), our proposed COG substantially outperforms standard
baselines on automatic metrics (26.14 vs. 23.43 MAUVE (Pillutla et al., 2021)) and human evaluation
(48% vs. 28% human preference). Moreover, when we directly switch the text collection from the
WikiText-103 corpus to a domain-specific corpus, Law-MT (Koehn & Knowles, 2017), our proposed
COG outperforms strong baselines on this domain adaption setting (28.14 vs. 26.85 MAUVE and
52% vs. 36% human preference) without any domain-specific training. Furthermore, when we scale
up the text collection of COG to a larger one, the En-Wiki dataset, we obtain additional gain (26.97
vs. 23.43 MAUVE), again without any further training. Our contributions can be summarized as
follows:


• We propose COG, a method that reformulates text generation tasks as a series of copy-andpaste operations from existing text collections.

• We show that COG can outperform standard neural language model baselines on existing
language modeling benchmarks.

• We demonstrate that COG allows for training-free adaptations to larger text collections and
domain-specific text collections.

**
