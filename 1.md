In this section, we present the experimental results
for detecting experiment-describing sentences, entity mention extraction and experiment slot identification. For tokenization, we employ ChemDataExtractor,8 which is optimized for dealing with chemical formulas and unit mentions.
We tune our models in a 5-fold cross-validation
setting. We also report the mean and standard deviation across those folds as development results.
For the test set, we report the macro-average of
the scores obtained when applying each of the
five models to the test set. To put model performance in relation to human agreement, we report
the corresponding statistics obtained from our interannotator agreement study (Section 5). Note that
these numbers are based on a subset of the data and
are hence not directly comparable.
Hyperparameters and training. The BiLSTM
models are trained with the Adam optimizer
(Kingma and Ba, 2015) with a learning rate of
1e-3. For fine-tuning the original BERT models,
we follow the configuration published by Wolf et al.
(2019) and use AdamW (Loshchilov and Hutter,
2019) as optimizer and a learning rate of 4e-7 for
sentence classification and 1e-5 for sequence tagging. When adding BERT tokens to the BiLSTM,
we also use the AdamW optimizer for the whole
model and learning rates of 4e-7 or 1e-5 for the
BERT part and 1e-3 for the remainder. For regularization, we employ early stopping on the development set. We use a stacked BiLSTM with two
hidden layers and 500 hidden units for all tasks
with the exception of the experiment sentence de
tection task, where we found one BiLSTM layer
to work best. The attention layer of the sentence
detection model has a hidden size of 100.


