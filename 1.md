Section 4 describes our experimental setup.
To show the wide applicability of our framework, we performed LM experiments on a suite
of five diverse corpora: WikiText-103 (Merity
et al., 2016), RealNews (Zellers et al., 2019), and
three datasets from The Pile (Gao et al., 2021):
ArXiv, Stack Exchange, and FreeLaw. We use
open-source LMs ranging from 110M to 66B parameters (from the GPT-2, GPT-Neo, OPT, and
LLaMA model families).



In Section 5 we evaluate the application of
off-the-shelf retrievers to our framework. In this
minimal-effort setting, we found that In-Context
RALM led to LM performance gains equivalent
to increasing the LM’s number of parameters by
2–3× across all of the text corpora we examined. In Section 6 we investigate methods for
adapting document ranking to the LM task, a relatively under-explored RALM degree of freedom.
Our adaptation methods range from using a small
LM to perform zero-shot ranking of the retrieved
documents, up to training a dedicated bidirectional reranker by employing self-supervision
from the LM signal. These methods lead to further gains in the LM task corresponding to an
additional size increase of 2× in the LM architecture. As a concrete example of the gains, a
345M parameter GPT-2 enhanced by In-Context
RALM outperforms a 762M parameter GPT-2
when employing an off-the-shelf BM25 retriever
(Robertson and Zaragoza, 2009), and outperforms
a 1.5B parameter GPT-2 when employing our
trained LM-oriented reranker (see Figure 2). For
large model sizes, our method is even more effective: In-Context RALM with an off-the-shelf
retriever improved the performance of a 6.7B
parameter OPT model to match that of a 66B
parameter parameter OPT model (see Figure 4).

In Section 7 we demonstrate the applicability of
In-Context RALM to downstream open-domain
questions answering (ODQA) tasks.


In a concurrent work, Shi et al. (2023) also suggest to augment off-the-shelf LMs with retrieved
texts by prepending them to the input. Their results are based on training a dedicated retriever for
language modeling. In contrast, we focus on the
gains achievable in using off-the-shelf retrievers
for this task. We show strong gains of this simpler
setting by investigating: (1) which off-the-shelf
retriever is best suited for language modeling, (2)
the frequency of retrieval operations, and (3) the
optimal query length. In addition, we boost the
off-the-shelf retrieval performance by introducing
two reranking methods that demonstrate further
gains in perplexity


We believe that In-Context RALM can play
two important roles in making RALM systems
more powerful and more prevalent. First, given
its simple reading mechanism, In-Context RALM
can serve as a clean probe for developing document retrieval methods that are specialized for the
LM task. These in turn can be used to improve
both In-Context RALM and other more elaborate
RALM methods that currently leverage general
purpose retrievers. Second, due to its compatibility with off-the-shelf LMs, In-Context RALM can
help drive wider deployment of RALM systems.
