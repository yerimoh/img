Neural text generation can be divided into two categories: (1) unconditional text generation; (2)
conditional text generation. Unconditional text generation (or language modeling) aims to generate a
coherent text continuation given a prefix. In this case, language models perform generation using
a density estimation over sequences pθ(x). Conditional text generation aims to generate text with
some condition c and instead estimates the probability of pθ(x|c). Its typical applications include
machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), summarization (See et al., 2017).
Throughout this paper, our discussion will be focused on unconditional text generation, however, our
approach can be readily adapted to conditional text generation as well.


The canonical approach to language modeling factors the generation in an autoregressive left-to-right
manner pθ(x0:n) = Qn
i=1 p(xi
|x<i). In this case, text generation is reduced to the task of repeatedly
predicting the next token conditioned on the partial sequence (i.e., prefix) generated so far p(xi
|x<i).
The model often consists of two parts: (1) a prefix encoder and (2) a set of token embeddings. The
prefix encoder is often parameterized by the Transformer architecture (Vaswani et al., 2017), which
transforms any prefix into a fixed-sized vector representation hi ∈ R
d = PrefixEncoder(x<i). Then,
the probability of the next token being w is calculated as


where vw is the context-independent token embedding representing the token w, and V is the predefined vocabulary consisting of all possible tokens. Based on the chosen decoding method, such as
greedy search and nucleus sampling (Holtzman et al., 2020), the next token is selected according to
the probability distribution over the fixed vocabulary V . This process is repeated in an autoregressive
manner, until some stop condition is reached, e.g., the maximum length of generation is reached.
