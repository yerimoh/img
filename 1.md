Large language models (LLMs) excel as zero- and few-shot learners across various tasks (Brown
et al., 2020; Chowdhery et al., 2022; Touvron et al., 2023a;b; Anil et al., 2023; OpenAI, 2023).
However, because knowledge is represented only in the model parameters, they struggle to capture
long-tail knowledge (Tirumala et al., 2022; Sun et al., 2023) and require substantial resources to be
kept up-to-date (Miller, 2023). Retrieval-Augmented Language Modeling (RALM) integrates LLMs
with non-parametric information retrieval to overcome these limitations (Guu et al., 2020; Borgeaud
et al., 2022; Izacard et al., 2022b; Shi et al., 2023b; Ram et al., 2023). By explicitly decoupling
knowledge retrieval from the backbone language model, such architectures have exhibited superior
performance on knowledge intensive tasks such as open-domain question answering (Lewis et al.,
2020; Izacard et al., 2022b) and live chat interactions (Liu, 2022).



Existing efforts in RALM development primarily focus on two high-level challenges: (i) enhancing
the LLM’s capability to incorporate retrieved knowledge (Lewis et al., 2020; Izacard et al., 2022b;
Luo et al., 2023) and (ii) refining the retrieval component to return more relevant content (Shi et al.,
2023b; Izacard et al., 2022b). Retrieval capabilities have also been introduced at different stages
of the model training process. REALM (Guu et al., 2020) and RETRO (Borgeaud et al., 2022) opt
for end-to-end pre-training, incorporating the retrieval component from the outset. ATLAS (Izacard
et al., 2022b) builds upon the T5 language model (Raffel et al., 2020), and continuosly pre-trains
the framework over unsupervised text. REPLUG (Shi et al., 2023b) and In-Context RALM (Ram
et al., 2023) combine off-the-shelf LLMs with general-purpose retrievers, showing that LLMs and
retrievers, even when optimized independently, can be effectively fused through the emergent incontext learning capbabilities of LLMs. However, extensive pre-training of such architectures incurs
high computational costs, and the off-the-shelf fusion approach also has limitations, particularly as
the LLMs are not inherently trained to incorporate retrieved content.


In this work, we show lightweight instruction tuning (Chung et al., 2022b; Iyer et al., 2022; Zhou
et al., 2023) alone can significantly boost the performance of RALMs, especially in scenarios
that require access to large, external knowledge sources. We propose Retrieval-Augmented Dual
Instruction Tuning (RA-DIT), an approach that retrofits any LLM with retrieval capabilities via
fine-tuning over a set of tasks selected to cultivate knowledge utilization and contextual awareness
in the language model predictions. We initialize the framework using pre-trained LLAMA (Touvron et al., 2023a) and a state-of-the-art dual-encoder based dense retriever, DRAGON+ (Lin et al.,
2023). Following Shi et al. (2023b), we retrieve relevant text chunks based on the language model
prompt. Each retrieved chunk is prepended to the prompt, and the predictions from multiple chunks
are computed in parallel and ensembled to produce the final output.




We perform instruction-tuning in two separate steps. For language model fine-tuning (LM-ft), we
adopt the supervised fine-tuning objective (Chung et al., 2022b; Iyer et al., 2022) while augmenting
each fine-tuning prompt with a retrieved “background” field prepended to the instructions (Figure 1).
We also leverage the design of existing NLP tasks and populate this field with the ground truth context for tasks such as reading comprehension and summarization. By incorporating the background
text during fine-tuning, we guide the LLM to optimally utilize the retrieved information and ignore
distracting content (Shi et al., 2023a). For retriever fine-tuning (R-ft), we update the query encoder
using a generalized LM-Supervised Retrieval (LSR, Shi et al., 2023b) training objective computed
over a combination of supervised tasks and unsupervised text completion. This way we enable the
retriever to yield more contextually relevant results, aligned with the preferences of the LLM.



We demonstrate that each fine-tuning step offers significant performance gains, and that the finetuned LLM and retriever can be combined to achieve further improvements. Our largest model,
RA-DIT 65B, attains state-of-the-art performance in zero- and few-shot settings on knowledge
intensive benchmarks, notably surpassing the un-tuned in-context RALM approach on datasets
including MMLU (Hendrycks et al., 2021b) (+8.2% 0-shot; +0.7% 5-shot) and Natural Questions (Kwiatkowski et al., 2019) (+22% 0-shot; +3.8% 5-shot). In addition, RA-DIT 65B also
substantially outperforms ATLAS 11B on 8 knowledge-intensive tasks (+7.2% on average in the
64-shot fine-tuning setting). This suggests that language models and retrievers, when optimized independently and then fused through instruction-tuning, can compete effectively with RALMs that
have undergone extensive continuous pre-training. We further conduct a comprehensive model analysis, showing the effectiveness of our approach across LLMs of varying sizes, as well as evaluating
the influence of different fine-tuning strategies and retriever configurations.


---

7 CONCLUSION


In this paper, we propose RA-DIT, a lightweight Retrieval-Augmented Dual Instruction Tuning
framework that can effectively retrofit any pre-trained LLM with retrieval capabilities. RA-DIT
updates the LLM with retrieval-augmented instruction tuning to make better use of retrieved knowledge and ignore irrelevant or distracting information. It also fine-tunes the retriever with supervision from the LLM to retrieve texts that can better help the LLM generate correct outputs. RADIT achieves state-of-the-art performance in zero- and few-shot evaluations on knowledge intensive
benchmarks, surpassing un-tuned in-context RALM approaches such as REPLUG and compete effectively against methods that require extensive pre-training such as ATLAS.
