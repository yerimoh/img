8 Conclusion

In this work, we perform a comprehensive study of
pretrained retrieval-augmented LLM to answer the
question: Shall we pretrain decoder-only LMs with
retrieval? We observe consistent improvements in
text generation quality, factual accuracy, lower toxicity, and downstream task accuracy, especially for
knowledge-intensive tasks, including open-domain
QA. Given the ∼ 25% percentage of additional
GPU hours for pretraining (see Table 11 Appendix
B), we argue pretraining generative language models with retrieval is a promising direction.


Limitations

Despite the impressive performance of RETRO and
RETRO++, our findings reveal several limitations
that pave the way for future research to address:

• The quality of the retrieval database

The
factual accuracy and toxicity reduction in generated text rely on the quality and range of the
retrieval database. This means that the performance and the model’s outputs can vary based
on the retrieval database. The performance of
RETRO could be compromised if the database
contains inaccurate, biased, or outdated information.

Scalability




The pretraining of GPT and
retrieval-augmented LLM from scratch requires
significant computational resources. Our work
follows Borgeaud et al. (2022) and pretrains
GPT and RETRO up to the size of 9B. We leave
it as an important future work to further scale
up the size of retrieval-augmented LLMs.









