Retrieval-augmented language models present viable solutions to address issues like hallucinations in
large language models, but their inference is slow because the reader component needs to process
many passages. We introduce BTR, a system that creates cacheable binary token representations
to improve the inference and storage efficiency for the reader in retrieval-augmented language
models. Together with training regularization techniques and token compression methods, BTR
effectively reduces the storage costs by 100x, improves inference throughput by 2âˆ¼4x, and maintains
performance for a wide range of knowledge-intensive tasks



We discuss directions for future work. 1) Extending BTR to decoder-only readers is non-trivia because
decoder models compute passage representations together with the query in a sequential manner,
making it challenging to break computational dependencies and cache passage representations.
Moreover, the KV-Caches in decoder models speed up the inference decoding, but storing their binary
representation causes much more storage than encoder models. 2) Improving BTR for extremely long
input queries remains challenging and requires other orthogonal efficient methods. BTR can speed
up the inference when queries are longer, but the speed-up is relatively smaller than shorter queries.
3) Scaling BTR for larger models with bigger representation sizes is another important research topic.
Potential solutions might include using autoencoders to compress the dimension of representations.
4) It will also be interesting to apply binary token representations to the retriever and incorporate
BTR into model pretraining for building better and faster retrieval-augmented language models.
