1. Introduction

2. 
 Transfer from pre-trained models yields strong performance
 on many NLP tasks (Dai & Le, 2015; Howard & Ruder,
 2018; Radford et al., 2018). BERT, a Transformer network
 trained on large text corpora with an unsupervised loss,
 attained state-of-the-art performance on text classification
 and extractive question answering (Devlin et al., 2018).



 In this paper we address the online setting, where tasks
 arrive in a stream. The goal is to build a system that per
forms well on all of them, but without training an entire new
 model for every new task. A high degree of sharing between
 tasks is particularly useful for applications such as cloud
 services, where models need to be trained to solve many
 tasks that arrive from customers in sequence. For this, we
 propose a transfer learning strategy that yields compact and
 extensible downstream models. Compact models are those
 that solve many tasks using a small number of additional
 parameters per task. Extensible models can be trained in
crementally to solve new tasks, without forgetting previous
 ones. Our method yields a such models without sacrificing
 performance.


 The two most common transfer learning techniques in NLP
 are feature-based transfer and fine-tuning. Instead, we
 present an alternative transfer method based on adapter
 modules (Rebuffi et al., 2017). Features-based transfer in
volves pre-training real-valued embeddings vectors. These
 embeddings may be at the word (Mikolov et al., 2013), sen
tence (Cer et al., 2019), or paragraph level (Le & Mikolov,
 2014). The embeddings are then fed to custom downstream
 models. Fine-tuning involves copying the weights from a
 pre-trained network and tuning them on the downstream
 task. Recent work shows that fine-tuning often enjoys better
 performance than feature-based transfer (Howard & Ruder,
 2018).


 Both feature-based transfer and fine-tuning require a new
 set of weights for each task. Fine-tuning is more parameter
 efficient if the lower layers of a network are shared between
 tasks. However, our proposed adapter tuning method is even
 more parameter efficient. Figure 1 demonstrates this trade
off. The x-axis shows the number of parameters trained per
 task; this corresponds to the marginal increase in the model
 size required to solve each additional task. Adapter-based
 tuning requires training two orders of magnitude fewer pa
rameters to fine-tuning, while attaining similar performance




Adapters are new modules added between layers of a
 pre-trained network. Adapter-based tuning differs from
 feature-based transfer and fine-tuning in the following way.
 Consider a function (neural network) with parameters w:
 w(x). Feature-based transfer composes w with a new
 function, v, to yield v( w(x)). Only the new, task
specific, parameters, v, are then trained. Fine-tuning in
volves adjusting the original parameters, w, for each new
 task, limiting compactness. For adapter tuning, a new
 function, wv(x), is defined, where parameters w are
 copied over from pre-training. The initial parameters v0
 are set such that the new function resembles the original:
 wv0
 (x) 
w(x). During training, only v are tuned.
 For deep networks, defining wv typically involves adding
 new layers to the original network, w. If one chooses
 v
 w,the resulting model requires 
wparameters
 for many tasks. Since w is fixed, the model can be extended
 to new tasks without affecting previous ones.



 Adapter-based tuning relates to multi-task and continual
 learning. Multi-task learning also results in compact models.
 However, multi-task learning requires simultaneous access
 to all tasks, which adapter-based tuning does not require.
 Continual learning systems aim to learn from an endless
 stream of tasks. This paradigm is challenging because net
works forget previous tasks after re-training (McCloskey
 &Cohen, 1989; French, 1999). Adapters differ in that the
 tasks do not interact and the shared parameters are frozen.
 This means that the model has perfect memory of previous
 tasks using a small number of task-specific parameters.


  Wedemonstrate on a large and diverse set of text classifica
tion tasks that adapters yield parameter-efficient tuning for
 NLP. The key innovation is to design an effective adapter
 module and its integration with the base model. We propose
 a simple yet effective, bottleneck architecture. On the GLUE
 benchmark, our strategy almost matches the performance of
 the fully fine-tuned BERT, but uses only 3% task-specific
 parameters, while fine-tuning uses 100% task-specific pa
rameters. We observe similar results on a further 17 public
 text datasets, and SQuAD extractive question answering. In
 summary, adapter-based tuning yields a single, extensible,
 model that attains near state-of-the-art performance in text
 classification.
