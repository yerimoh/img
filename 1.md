The self-supervised objective of masking-andpredicting has led to promising performance
gains on a variety of downstream tasks. However, while most approaches randomly mask
tokens, there is strong intuition that deciding
what to mask can substantially improve learning outcomes. We investigate this in continued
pretraining setting in which pretrained models
continue to pretrain on domain-specific data before performing some downstream task. We
introduce DIFFERENCE-MASKING, a masking strategy that automatically chooses what
to mask during continued pretraining by considering what makes a task domain different
from the pretraining domain. Empirically,
we find that DIFFERENCE-MASKING outperforms baselines on continued pretraining settings across four diverse language-only and
multimodal video tasks.




1 Introduction

Inspired by the distributional hypothesis in the language domain (Harris, 1954), masking is a selfsupervised learning (SSL) objective in which a
model attempts to reconstruct hidden portions of
data from the surrounding context. Masking has
enabled breakthrough performance on tasks from a
variety of domains, such as language, vision, and
speech (Devlin et al., 2019; Li et al., 2021; Hsu
et al., 2021; Ericsson et al., 2022), motivating interest in researching how masking strategies influence
representation learning in SSL.




Masked prediction has recently been applied to
adapt pretrained models to specific downstream
tasks by continuing to pretrain models on indomain unlabelled data (Dery et al., 2023). Masking in this continued pretraining setting been shown
to be particularly effective when the target domain differs substantially from the pretraining domain (Gururangan et al., 2020).



While prior work has studied how the amount
masked influences model learning (He et al., 2022)
most masking approaches randomly choose which
parts of the data to mask. Although it is understudied in SSL, deciding what to mask is a critical
component in human education (Pajares and Miller,
1997; Bjork and Linn, 2006). Educators designing
“fill-in-the-blank” assessments for students must decide what content to mask in order to effectively
assess student understanding of a domain (Bae and
Lee, 2018). For example, in a real-world “fill-inthe-blank” chemistry test, a teacher might choose
to mask domain-specific words (“density”, “silicon”) to assess student learning, instead of masking
domain-irrelevant words (“example”, “process”).



In this paper, we propose DIFFERENCEMASKING, a novel approach for automatically selecting what to mask during continued pretraining.
Our strategy first identifies anchors that describe
what makes a target domain different from the pretraining domain and then determines what to mask
during continued pretraining based on similarity to
those anchors






In experiments spanning four diverse languageonly and multimodal video datasets (ACL-ARC,
ChemProt, TVQA, and Social-IQ), we find that
DIFFERENCE-MASKING outperforms strong baselines, supporting our hypothesis that masking based
on what is different about a task provides strong
representation for continued pretraining. We provide intuitions to explain the strong performance
of DIFFERENCE-MASKING, along with extensive
analyses and ablations to better understand the performance of our method. Our code is publicly
available.






























