The advent of powerful NLP models has enabled
the analysis and generation of text-based data
across a variety of domains. BERT (Devlin et al.,
2018) was one of the first large-scale transformerbased models to substantially advance the state-ofthe-art by training on large amounts of unlabeled
text data in a self-supervised way. The pretraining procedure was followed by task-specific finetuning, leading to impressive results on a variety of
NLP task, such as named entity recognition (NER),
question and answering (QA), and relation classification (Hakala and Pyysalo, 2019; Qu et al., 2019;
Wu and He, 2019). A significant collection of large
language models spanning millions to billions of
parameters followed the success of BERT adopting
a similar approach of pretraining on vast corpora
of text with task-specific fine-tuning to push the
state-of-the-art for in natural language processing
and understanding (Raffel et al., 2020; Brown et al.,
2020; Scao et al., 2022).



2.1 Scientific Language Models
The success of large language models on general text motivated the development of domainspecific language models pretrained on custom text
data, including text in the scientific domain: SciBERT (Beltagy et al., 2019), ScholarBERT (Hong
et al., 2022) and Galactica (Taylor et al., 2022)
are pretrained on general corpus of scientific articles; BioBERT (Lee et al., 2020), PubMedBERT
(Gu et al., 2021), BioMegatron (Shin et al., 2020)
and Sci-Five (Phan et al., 2021) are pretrained on
various kinds of biomedical corpora; MatBERT
(Walker et al., 2021), MatSciBERT (Gupta et al.,
2022) are pretrained on materials science specific
corpora; and BatteryBERT (Huang and Cole, 2022)
is pretrained on a corpus focused on batteries.
Concurrently, several domain-specific NLP benchmarks were established to assess language model
performance on domain-specific tasks, such as
QASPER (Dasigi et al., 2021) and BLURB (Gu
et al., 2021) in the scientific domain, as well as
PubMedQA (Jin et al., 2019), BioASQ (Balikas
et al., 2015), and Biomedical Language Understanding Evaluation (BLUE) (Peng et al., 2019)
in the biomedical domain.


2.2 NLP in Materials Science
The availability of openly accessible, high-quality
corpora of materials science text data remains
highly restricted in large part because data from
peer-reviewed journals and scientific documents
is usually subject to copyright restrictions, while
open-domain data is often only available in
difficult-to-process PDF formats (Olivetti et al.,
2020; Kononova et al., 2021). Moreover, specialized scientific text, such as materials synthesis procedures containing chemical formulas and reaction
notation, require advanced data mining techniques
for effective processing (Kuniyoshi et al., 2020;
Wang et al., 2022b). Given the specificity, complexity, and diversity of specialized language in
scientific text, effective extraction and processing
remain an active area of research with the goal
of building relevant and sizeable text corpora for
pretraining scientific language models (Kononova
et al., 2021).



Nonetheless, materials science-specific language
models, including MatBERT (Walker et al., 2021),
MatSciBERT (Gupta et al., 2022), and BatteryBERT (Huang and Cole, 2022), have been trained
on custom-built pretraining dataset curated by different academic research groups. The pretrained
models and some of the associated fine-tuning data
have been released to the public and have enabled
further research, including this work.



The nature of NLP research in materials science
to date has also been highly fragmented with many
research works focusing on distinct tasks motivated
by a given application or methodology. Common
ideas among many works include the prediction
and construction of synthesis routes for a variety
of materials (Mahbub et al., 2020; Karpovich et al.,
2021; Kim et al., 2020), as well as the creation of
novel materials for a given application (Huang and
Cole, 2022; Georgescu et al., 2021; Jensen et al.,
2021), both of which relate broader challenges in
the field of materials science.





