We present MatSci-NLP, a natural language
benchmark for evaluating the performance of
natural language processing (NLP) models on
materials science text. We construct the benchmark from publicly available materials science
text data to encompass seven different NLP
tasks, including conventional NLP tasks like
named entity recognition and relation classification, as well as NLP tasks specific to materials science, such as synthesis action retrieval which relates to creating synthesis procedures for materials. We study various BERTbased models pretrained on different scientific text corpora on MatSci-NLP to understand the impact of pretraining strategies on
understanding materials science text. Given
the scarcity of high-quality annotated data
in the materials science domain, we perform
our fine-tuning experiments with limited training data to encourage the generalize across
MatSci-NLP tasks. Our experiments in this
low-resource training setting show that language models pretrained on scientific text outperform BERT trained on general text. MatBERT, a model pretrained specifically on materials science journals, generally performs
best for most tasks. Moreover, we propose a
unified text-to-schema for multitask learning
on MatSci-NLP and compare its performance
with traditional fine-tuning methods. In our
analysis of different training methods, we find
that our proposed text-to-schema methods inspired by question-answering consistently outperform single and multitask NLP fine-tuning
methods. The code and datasets are publicly
available1



1 Introduction

Materials science comprises an interdisciplinary
scientific field that studies the behavior, properties
and applications of matter that make up materials
systems. As such, materials science often requires
deep understanding of a diverse set of scientific disciplines to meaningfully further the state of the art.
This interdisciplinary nature, along with the great
technological impact of materials advances and
growing research work at the intersection of machine learning and materials science (Miret et al.;
Pilania, 2021; Choudhary et al., 2022), makes the
challenge of developing and evaluating natural language processing (NLP) models on materials science text both interesting and exacting.


The vast amount of materials science knowledge
stored in textual format, such as journal articles,
patents and technical reports, creates a tremendous
opportunity to develop and build NLP tools to create and understand advanced materials. These tools
could in turn enable faster discovery, synthesis and
deployment of new materials into a wide variety
of application, including clean energy, sustainable
manufacturing and devices.


Understanding, processing, and training language models for scientific text presents distinctive
challenges that have given rise to the creation of
specialized models and techniques that we review
in Section 2. Additionally, evaluating models on
scientific language understanding tasks, especially
in materials science, often remains a laborious task
given the shortness of high-quality annotated data
and the lack of broad model benchmarks. As such,
NLP research applied to materials science remains
in the early stages with a plethora of ongoing research efforts focused on dataset creation, model
training and domain specific applications.



The broader goal of this work is to enable the
development of pertinent language models that can
be applied to further the discovery of new material
systems, and thereby get a better sense of how well
language models understand the properties and behavior of existing and new materials. As such, we
propose MatSci-NLP, a benchmark of various NLP
tasks spanning many applications in the materials
science domain described in Section 3. We utilize
this benchmark to analyze the performance of various BERT-based models for MatSci-NLP tasks
under distinct textual input schemas described in
Section 4. Concretely, through this work we make
the following research contributions:


• MatSci-NLP Benchmark: We construct the
first broad benchmark for NLP in the materials science domain, spanning several different
NLP tasks and materials applications. The
benchmark contents are described in Section 3
with a general summary and data sources provided in Table 1. The processed datasets and
code will be released after acceptance of the
paper for reproducibility.


Text-to-Schema Multitasking: We develop
a set of textual input schemas inspired by
question-answering settings for fine-tuning
language models. We analyze the models’
performance on MatSci-NLP across those settings and conventional single and multitask
fine-tuning methods. In conjunction with this
analysis, we propose a new Task-Schema input format for joint multitask training that increases task performance for all fine-tuned
language models.


• MatSci-NLP Analysis: We analyze the performance of various BERT-based models
pretrained on different scientific and nonscientific text corpora on the MatSci-NLP
benchmark. This analysis help us better understand how different pretraining strategies
affect downstream tasks and find that MatBERT (Walker et al., 2021), a BERT model
trained on materials science journals, generally performs best reinforcing the importance
of curating high-quality pretraining corpora.



We centered our MatSci-MLP analysis on exploring the following questions:


Q1 How does in-domain pretraining of language
models affect the downstream performance on
MatSci-NLP tasks? We investigate the performance of various models pretrained on different kinds of domain-specific text including
materials science, general science and general language (BERT (Devlin et al., 2018)).
We find that MatBERT generally performs
best and that language models pretrained on
diverse scientific texts outperform a general
language BERT. Interestingly, SciBERT (Beltagy et al., 2019) often outperforms materials
science language models, such as MatSciBERT (Gupta et al., 2022) and BatteryBERT
(Huang and Cole, 2022).
Q2 How do in-context data schema and multitasking affect the learning efficiency in lowresource training settings? We investigate
how several input schemas shown in Figure 1
that contain different kinds of information affect various domain-specific language models and propose a new Task-Schema method.
Our experiments show that our proposed TaskSchema method mostly performs best across
all models and that question-answering inspired schema outperform single task and multitask fine-tuning settings.




