 Identification of experiment-describing sen
tences. Agreement on our first task, judging
 whetherasentencecontainsrelevantexperimental
 information,is0.75intermsofCohen’s (Cohen,
 1968),indicatingsubstantialagreementaccording
 toLandisandKoch(1977).Theobservedagree
ment, corresponding toaccuracy, is94.9%; ex
pectedagreementamountsto79.2%.Table2shows
 precision,recallandF1forthedoubly-annotated
 subset,treatingoneannotatorasthegoldstandard
 andtheotherone’slabelsaspredicted. Ourpri
maryannotatoridentifies119outof973sentences
 asexperiment-describing,oursecondaryannota
tor111sentences,withanoverlapof90sentences.
 Thesestatisticsarehelpfultogainfurtherintuition
 ofhowwellahumancanreproduceanotheranno
tator’slabelsandcanalsobeconsideredanupper
 boundforsystemperformance.

 Modeling
 
  Inthissection,wedescribeasetofneural-network
 basedmodelarchitecturesfortacklingthevarious
 informationextractiontasksdescribedinSection4.
 Experimentdetection. Thetaskofexperiment
 detectioncanbemodeledasabinarysentenceclas
sificationproblem. It canalsobeconceivedas
 aretrievaltask,selectingsentencesascandidates
 forexperimentframeextraction.Weimplementa
 bidirectionallongshort-termmemory(BiLSTM)
 model with attention for the task of experiment sen
tence detection. Each input token is represented by
 a concatenation of several pretrained word embed
dings, each of which is fine-tuned during training.
 We use the Google News word2vec embeddings
 (Mikolov et al., 2013), domain-specific word2vec
 embeddings (mat2vec, Tshitoyan et al., 2019, see
 also Section 2), subword embeddings based on
 byte-pair encoding (bpe, Heinzerling and Strube,
 2018), BERT (Devlin et al., 2019), and SciBERT
 (Beltagy et al., 2019) embeddings. For BERT and
 SciBERT, we take the embeddings of the first word
 piece as token representation. The embeddings
 are fed into a BiLSTM model followed by an at
tention layer that computes a vector for the whole
 sentence. Finally, a softmax layer decides whether
 the sentence contains an experiment.


 In addition, we fine-tune the original (uncased)
 BERT (Devlin et al., 2019) as well as SciBERT
 (Beltagy et al., 2019) models on our dataset. Sci
BERT was trained on a large corpus of scientific
 text. We use the implementation of the BERT sen
tence classifier by Wolf et al. (2019) that uses the
 CLS token of BERT as input to the classification
 layer.5

Finally, we compare the neural network mod
els with traditional classification models, namely
 a support vector machine (SVM) and a logistic re
gression classifier. For both models, we use the fol
lowing set of input features: bag-of-words vectors
 indicating which 1- to 4-grams and part-of-speech
 tags occur in the sentence.6
 


 
