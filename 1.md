We first describe the architecture of retrieval-augmented language models and then related efficiency
techniques designed for them. Next, we summarize relevant methods to improve model efficiency,
including quantization and binary representations.



Retrieval-Augmented Language Models. 

Retrieval-augmented language models have shown
strong performance in many tasks, including language modeling (Borgeaud et al., 2022; Min et al.,
2023), open-domain question answering, and fact checking (Izacard et al., 2022b; Lewis et al., 2020;
Guu et al., 2020; Shi et al., 2023).


As shown in Figure 1, a retrieval-augmented language model works by first using a retriever to retrieve
many passages relevant to the input query and then using a reader component to extract or generate
the answers. While the retriever is often fast enough, the reader causes a speed bottleneck in retrieval
augmentation because it requires computing cross-attention between the input query and many
passages. For example, Figure 1 illustrates a state-of-the-art encoder-decoder reader (Izacard et al.,
2022b) architecture called Fusion-in-Decoder (FiD) (Izacard & Grave, 2021). FiD first concatenates
each passage with the query and processes them in parallel (independently) in the encoder; then the
decoder fuses information across all the concatenated passage-query representations and produces
answers. In our experiments, we observe that the passage encoding takes over 60% of the reader’s
computation on commodity GPUs, and we save such computations by precomputing the passage
representations, leading to significant speedup in the inference with reduced storage costs.


Efficient methods for reader models.

DensePhrase (Lee et al., 2021) builds contextualized phrase
representations for the passage corpus, completely removes the reader component, and uses phrase
retrieval to produce answers for the query. Despite its high inference throughput, the accuracy is
much lower than similar size BTR models. FiD-light (Hofstätter et al., 2022) and FiDO (de Jong et al.,
2022) focus on improving the inference latency for FiD models on customized hardware like TPUs,
they compress passage representations into fixed-size vectors to reduce the decoder computation.
But on more popular hardware like GPUs, the passage encoder is the computation bottleneck, and
their decoder-oriented optimizations are likely to be less effective in improving inference speed.
LUMEN (de Jong et al., 2023) and DeFormer (Cao et al., 2020) precompute cacheable continuous
passage representations to speed up reader inference, but they take up 100x more storage than BTR.
