Large language models (LLMs) (Brown et al., 2020b; Touvron et al., 2023a), despite their widespread
success, still suffer from issues such as hallucinations (Mallen et al., 2023; Mündler et al., 2023),
staleness, and privacy leaks (Huang et al., 2022). Retrieval-augmented language models (e.g., (Lewis
et al., 2020; Izacard et al., 2022b)) alleviate such problems via a retrieve-and-read approach (Figure 1),
using a retrieval component to find passages relevant to the input query followed by a reader model
(e.g., an LLM) that generates output given the query concatenated with retrieved passages. However,
this approach is slow (e.g., only handles a few queries for a powerful server GPU) at inference time
mainly due to the reader model that needs to compute cross-attention between the input query and a
large number of passages. Existing solutions (Lee et al., 2021; Cao et al., 2020) improve the inference
computation by decomposing query and passage encoding and precomputing passage representations,
but they come with large storage costs (terabytes of storage; §4).


In this work, we improve the inference speed of the reader model in a retrieval-augmented LM with a
small storage footprint by introducing cacheable and calibrated Binary Token Representations (BTR).
BTR (Figure 1) precomputes token representations for the retrieved passages in the reader model. The
binary representations are 1-bit vectors for the tokens in a passage and are created from the hidden
states in the reader encoder layers via calibrated binarization which are effective for downstream
tasks such as question answering. BTR reduces the storage footprint and improves the runtime speed
since the representations are 1-bit vectors, and the reader uses the cached representations. To avoid
degradation of task accuracy caused by binary representations, we introduce two training objectives by
adding (i) a passage representation recovery objective that makes the binary representations preserve
the passage semantics before the binarization; and (ii) a query-aware passage token distillation
objective that compensates the information loss due to precompuation of passage representations
independent of the query


Furthermore, we observe significant redundancies in precomputed token representations in retrieved
passages since they are relevant to the query and contain similar information. Removing such redundancies only causes minimal task accuracy loss (<0.5% in our experiments), but shows significant
benefits in storage and runtime inference. To address this, we further develop token compression over
BTR by merging similar precomputed token vectors after training and merging similar concatenated
query-passage representations during inference.



We evaluate BTR on five knowledge-rich NLP tasks, including three open-domain questionanswering tasks (NaturalQuestions (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017),
and WebQA (Berant et al., 2013)), the FEVER (Thorne et al., 2018) fact-checking task, and the
MMLU (Hendrycks et al., 2020) reasoning benchmark. Compared to baseline systems, BTR reduces
disk storage by up to 101x and improves inference speed by 2–4x for the reader of two state-of-the-art
retrieval-augmented language models. BTR also retains 90–95% of the original models’ performance.
Our analysis experiments show that binary token representations are effective and contribute most
to improving the inference speed and reducing the storage costs of readers. The training regularization objectives help mitigate the task accuracy loss and the offline and runtime token compression
techniques further bring down storage footprint and increase inference efficiency.











