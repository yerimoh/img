Through the creation of MatSci-NLP, we aim to
bring together some of the fragmented data across
multiple research works for a wide-ranging materials science NLP benchmark. As described in
Section 2, the availability of sizeable, high-quality
and diverse datasets remain a major obstacle in applying modern NLP to advance materials science
in meaningful ways. This is primarily driven by a
high cost of data labeling and the heterogeneous
nature of materials science. Given those challenges,
we created MatSci-NLP by unifying various publicly available, high-quality, smaller-scale datasets
to form a benchmark for fine-tuning and evaluating
modern NLP models for materials science applications. MatSci-NLP consists of seven NLP tasks
shown in Table 1, spanning a wide range of materials categories including fuel cells (Friedrich et al.,
2020), glasses (Venugopal et al., 2021), inorganic
materials (Weston et al., 2019; MatSciRE, 2022),
superconductors (Yamaguchi et al., 2020), and synthesis procedures pertaining to various kinds of
materials (Mysore et al., 2019; Wang et al., 2022a).
Some tasks in MatSci-NLP had multiple source
components, meaning that the data was curated
from multiple datasets (e.g. NER), while many
were obtained from a single source dataset.



The data in MatSci-NLP adheres to a standard
JSON-based data format with each of the samples
containing relevant text, task definitions, and annotations. These can in turn be refactored into
different input schemas, such as the ones shown
in Figure 1 consisting of 1) Input: primary text
jointly with task descriptions and instructions, and
2) Output: query and label, which we perform
in our text-to-schema modeling described in Section 4. Next, we describe the tasks in MatSci-NLP
in greater detail:


• Named Entity Recognition (NER): 

The
NER task requires models to extract summarylevel information from materials science text
and recognize entities including materials, descriptors, material properties, and applications
amongst others. The NER task predicts the
best entity type label for a given text span
si with a non-entity span containing a “null”
label. MatSci-NLP contains NER task data
adapted from Weston et al. (2019); Friedrich
et al. (2020); Mysore et al. (2019); Yamaguchi
et al. (2020).


 Relation Classification

In the relation classification task, the model predicts the most
 relevant relation type for a given span pair
(si
, sj ). MatSci-NLP contains relation classification task data adapted from Mysore et al.
(2019); Yamaguchi et al. (2020); MatSciRE
(2022).


• Event Argument Extraction: 

The event
argument extraction task involves extracting
event arguments and relevant argument roles.
As there may be more than a single event for
a given text, we specify event triggers and
require the language model to extract corresponding arguments and their roles. MatSciNLP contains event argument extraction task
data adapted from Mysore et al. (2019); Yamaguchi et al. (2020).






• Paragraph Classification: 


In the paragraph
classification task adapted from Venugopal
et al. (2021), the model determines whether a
given paragraph pertains to glass science.




• Synthesis Action Retrieval (SAR): 


SAR is
a materials science domain-specific task that
defines eight action terms that unambiguously
identify a type of synthesis action to describe
a synthesis procedure. MatSci-NLP adapts
SAR data from Wang et al. (2022a) to ask
language models to classify word tokens into
pre-defined action categories.




• Sentence Classification: 


In the sentence
classification task, models identify sentences
that describe relevant experimental facts based
on data adapted from Friedrich et al. (2020).


• Slot Filling: 



In the slot-filling task, models
extract slot fillers from particular sentences
based on a predefined set of semantically
meaningful entities. In the task data adapted
from Friedrich et al. (2020), each sentence describes a single experiment frame for which
the model predicts the slots in that frame.

