We show that adapters achieve parameter efficient transfer
for text tasks. On the GLUE benchmark (Wang et al., 2018),
adapter tuning is within 0.4% of full fine-tuning of BERT,
but it adds only 3% of the number of parameters trained by
fine-tuning. We confirm this result on a further 17 public
classification tasks and SQuAD question answering. Analysis shows that adapter-based tuning automatically focuses
on the higher layers of the network.


3.1. Experimental Settings

We use the public, pre-trained BERT Transformer network
as our base model. To perform classification with BERT,
we follow the approach in Devlin et al. (2018). The first
token in each sequence is a special “classification token”.
We attach a linear layer to the embedding of this token to
predict the class label.


Our training procedure also follows Devlin et al. (2018).
We optimize using Adam (Kingma & Ba, 2014), whose
learning rate is increased linearly over the first 10% of the
steps, and then decayed linearly to zero. All runs are trained
on 4 Google Cloud TPUs with a batch size of 32. For each
dataset and algorithm, we run a hyperparameter sweep and
select the best model according to accuracy on the validation
set. For the GLUE tasks, we report the test metrics provided
by the submission website2
. For the other classification
tasks we report test-set accuracy.





We compare to fine-tuning, the current standard for transfer
of large pre-trained models, and the strategy successfully
used by BERT. For N tasks, full fine-tuning requires N×
the number of parameters of the pre-trained model. Our
goal is to attain performance equal to fine-tuning, but with
fewer total parameters, ideally near to 1×.




3.2. GLUE benchmark



We first evaluate on GLUE.3 For these datasets, we transfer from the pre-trained BERTLARGE model, which contains 24 layers, and a total of 330M parameters, see Devlin
et al. (2018) for details. We perform a small hyperparameter sweep for adapter tuning: We sweep learning rates
in {3 · 10−5
, 3 · 10−4
, 3 · 10−3}, and number of epochs
in {3, 20}. We test both using a fixed adapter size (number of units in the bottleneck), and selecting the best size
per task from {8, 64, 256}. The adapter size is the only
adapter-specific hyperparameter that we tune. Finally, due
to training instability, we re-run 5 times with different random seeds and select the best model on the validation set.
Table 1 summarizes the results. Adapters achieve a mean
GLUE score of 80.0, compared to 80.4 achieved by full
fine-tuning. The optimal adapter size varies per dataset. For
example, 256 is chosen for MNLI, whereas for the smallest
dataset, RTE, 8 is chosen. Restricting always to size 64,
leads to a small decrease in average accuracy to 79.6. To
solve all of the datasets in Table 1, fine-tuning requires 9×
the total number of BERT parameters.4
In contrast, adapters
require only 1.3× parameters.
















