3 Convolutional Neural Network for
Relation Extraction
Our convolutional neural network for relation extraction consists of four main layers: (i) the look-up
tables to encode words in sentences by real-valued
vectors, (ii) the convolutional layer to recognize ngrams, (iii) the pooling layer to determine the most
relevant features and (iv) a logistic regression layer
(a fully connected neural network with a softmax at
the end) to perform classification (Collobert et al.,
2011; Kim, 2014; Kalchbrenner et al., 2014). Figure 1 gives an overview of the network.
3.1 Word Representation
The input to the CNN for relation extraction consists
of sentences marked with the two entity mentions of
interest. As CNNs can only work with fixed length
inputs, we compute the maximal separation between
entity mentions linked by a relation and choose an
input width greater than this distance. We insure
that every input (relation mention) has this length
by trimming longer sentences and padding shorter
sentences with a special token.
Let n be the length of the relation mentions and
x = [x1, x2, . . . , xn] be some relation mention
where xi
is the i-th word in the mention. Also, let
xi1
and xi2
be the two heads of the two entity mentions of interest . Before entering the network, each
word xi
is first transformed into a vector ei by looking up the word embedding table W that can be initialized either by a random process or by some pretrained word embeddings. Besides, in order to embed the positions of the two entity heads as well as
the other words in the relation mention into the representation, for each word xi
, its relative distances to
the two entity heads i−i1 and i−i2 are also mapped
into real-value vectors di1
and di2
respectively using
a position embedding table D (initialized randomly)
(Collobert et al., 2011; Liu et al., 2013; Zeng et al.,
2014). Note that the relative distances only range
from −n + 1 to n − 1 so the position embedding
matrix D has size (2n − 1) × md (md is a hyperparameter indicating the dimensionality of the position
embedding vectors). Finally, the word embeddingsei and the position embeddings d1 and d2 are concatenated into a single vector xi = [ei
, di1
, di2
]
> to
represent the word xi
. As a result, the original sentence x can now be viewed as a matrix x of size
(me + 2md) × n where me is the dimensionality of
the word embedding vectors.
x = [x1, x2, . . . , xn]
3.2 Convolution
In the next step, the matrix x representing the input relation mention is fed into the convolutional
layer to extract higher level features. Given a
widow size w, a filter is seen as a weight matrix
f = [f1,f2, . . . ,fw] (fi
is a column vector of size
me + 2md). The core of this layer is obtained from
the application of the convolutional operator on the
two matrices x and f to produce a score sequence
s = [s1, s2, . . . , sn−w+1]:
si = g(
wX−1
j=0
f
>
j+1x
>
j+i + b)
where b is a bias term and g is some non-linear
function. This process can then be replicated for various filters with different window sizes to increase
the n-gram coverage of the model.
For relation extraction, we call the n-grams accompanied with relative positions of its words the
augmented n-grams. It is instructive to think about
the filter f as representing some hidden class of the
augmented n-grams and the scores si as measuring
the possibility the augmented n-gram at position i
belongs to the corresponding hidden class (although
these scores are not probabilities at all). The trained
weights of the filter f would then amount to a feature
detector that learns to recognize the hidden class of
the augmented n-grams (Kalchbrenner et al., 2014).
3.3 Pooling
The rationale of the pooling layer is to further abstract the features generated from the convolutional
layer by aggregating the scores for each filter to introduce the invariance to the absolute positions but
preserve the relative positions of the n-grams between themselves and the entity heads at the same
time. The popular aggregating function is max as
it bears responsibility for identifying the most important or relevant features from the score sequence.
Concretely, for each filter f, its score sequence s is
passed through the max function to produce a single
number: pf = max{s} = max{s1, s2, . . . sn−w+1}
which can be interpreted as estimating the possibility some augmented n-gram of the hidden class of f
appears in the context.
3.4 Regularization and Classification
In the final step, the pooling scores for every filter
are concatenated into a single feature vector z =
[p1, p2, . . . , pm] to represent the relation mention.
Here, m is the number of filters in the model and
pi
is the pooling score of the i-th filter. Before actually applying this feature vector, following (Kim,
2014; Hinton et al., 2012), we execute a dropout for
42
regularization by randomly setting to zero a proportion ρ of the elements of the feature vector3 z to produce the vector zd. The dropout vector zd is then
fed into a fully connected layer of standard neural
networks followed by a softmax layer in the end to
perform classification. The fully connected layer induces a weight matrix C as model parameters. At
test time, the unseen relation mentions are scored
using the feature vectors that are not dropped out.
We also rescale the weights whose l2-norms exceed
a hyperparameter as Kim (2014).
Overall, the parameters for the presented CNN
are: the word embedding matrix W, the position embedding matrix D, the m filter matrices,
the weight matrix C for the fully connected layer.
The gradients are computed using back-propagation
while training is done via stochastic gradient descent
with shuffled mini-batches and the AdaDelta update
rule (Zeiler, 2012; Kim, 2014).


For all the experiments below, we use: tanh for
the non-linear function, 150 filters for each window
size in the model and position embedding vectors
with dimensionality of md = 504
. Regarding the
other parameters, we use the same values as do Kim
(2014), i.e, the dropout rate ρ = 0.5, the mini-batch
size of 50, the hyperparameter for the l2 of 3.
Finally, we utilize the pre-trained word embeddings word2vec from Mikolov et al. (2013) which
have dimensionality of me = 300 and are trained on
100 billion words of Google News using the continuous bag-of-words architecture. These embeddings
are publicly available here5
. Vectors for the words
not included in the pre-trained embeddings are initialized randomly. Besides the word embeddings
word2vec, the model does not use any other NLP
toolkits or resources.




4.1 Hyperparameters and Resources
For all the experiments below, we use: tanh for
the non-linear function, 150 filters for each window
size in the model and position embedding vectors
with dimensionality of md = 504
. Regarding the
other parameters, we use the same values as do Kim
(2014), i.e, the dropout rate ρ = 0.5, the mini-batch
size of 50, the hyperparameter for the l2 of 3.
Finally, we utilize the pre-trained word embeddings word2vec from Mikolov et al. (2013) which
have dimensionality of me = 300 and are trained on
100 billion words of Google News using the continuous bag-of-words architecture. These embeddings
are publicly available here5
. Vectors for the words
not included in the pre-trained embeddings are initialized randomly. Besides the word embeddings
word2vec, the model does not use any other NLP
toolkits or resources.


