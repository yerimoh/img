We aim to improve zero-shot performance of LLMs
by training a prompt retriever to retrieve prompts1
for any given task input. Specifically, UPRISE decomposes the prompting process into two steps:
retrieve then predict. Given an input x, we first
retrieve a set of positive prompts P
+ from a preconstructed pool P:



Then we concatenate P
+ with x to form an input
sequence for a frozen LLM, which generates a predicted output:



Our objective is to optimize performance of y
P+
to match the target y by updating the retriever R.


Figure 2 compares prompt retrieval with typical
prompt engineering methods: prompt design adds
an engineered natural language prompt (Brown
et al., 2020; Wei et al., 2022b) and prompt tuning tunes a soft prompt (Liu et al., 2021; Lester
et al., 2021). In contrast, prompt retrieval tunes
a retriever to retrieve natural language prompts,
which is both interpretable and flexible. It uses
the language model itself to label each prompt in
the pool as positive/negative, and then tunes a retriever from this signal (Rubin et al., 2022). Such
fine-tuned prompt retrieval has demonstrated effectiveness in the task-specific scenario (Rubin et al.,
2022; Ye et al., 2023): a prompt retriever is tuned
on one or multiple specific tasks using the training sets as the prompt pool. The retriever is then
evaluated on the corresponding testing sets.






Our work is to achieve universality of the prompt
retriever, which means the fine-tuned retriever can
be directly used to retrieve prompts for unseen tasks
and various inference LLMs, without the need for
further tuning. We define the universality from two
perspectives: cross-task retrieval and cross-model
retrieval.


Cross-task retrieval. 




Considering the diversity
of tasks in real-world applications, we propose
cross-task retrieval to retrieve for task types on
which the prompt retriever has not been trained.
We simulate this setting by evaluating the prompt
retriever on unseen task types: various tasks are
grouped into different clusters based on their task
types, and we hold out each task cluster for evaluation while training the retriever on all remaining
clusters (Wei et al., 2022a).



Cross-model retrieval.


Due to the high cost of
tuning a prompt retriever with a large-scale LLM,
we propose evaluating the capability to generalize
from a small LLM to a large LLM. Specifically, we
use a relatively small LLM for tuning the retriever,
while using a much larger LLM for inference. Furthermore, we suggest exploring the transferability
between different LLM sources, as there are LLMs
developed by different companies or institutions.








