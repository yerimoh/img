Existing large language models (LLMs) can only afford fix-sized inputs due to the
input length limit, preventing them from utilizing rich long-context information
from past inputs. To address this, we propose a framework, Language Models
Augmented with Long-Term Memory (LONGMEM), which enables LLMs to
memorize long history. We design a novel decoupled network architecture with
the original backbone LLM frozen as a memory encoder and an adaptive residual
side-network as a memory retriever and reader. Such a decoupled memory design
can easily cache and update long-term past contexts for memory retrieval without
suffering from memory staleness. Enhanced with memory-augmented adaptation
training, LONGMEM can thus memorize long past context and use long-term
memory for language modeling. The proposed memory retrieval module can
handle unlimited-length context in its memory bank to benefit various downstream
tasks. Typically, LONGMEM can enlarge the long-form memory to 65k tokens
and thus cache many-shot extra demonstration examples as long-form memory for
in-context learning. Experiments show that our method outperforms strong longcontext models on ChapterBreak, a challenging long-context modeling benchmark,
and achieves remarkable improvements on memory-augmented in-context learning
over LLMs. The results demonstrate that the proposed method is effective in
helping language models to memorize and utilize long-form contents. Our code is
open-sourced at https://aka.ms/LongMem.


1 Introduction

Large language models (LLMs) have revolutionized natural language processing with great successes
in advancing the state-of-the-art on various understanding and generation tasks [DCLT19, RWC+19,
LOG+19, YDY+19, BMR+20, RSR+20]. Most LLMs benefit from self-supervised training over
large corpora via harvesting knowledge from fix-sized local context, showing emergent abilities,
e.g., zero-shot prompting [RWC+19], in-context learning [BMR+20], and Chain-of-Thought (CoT)
reasoning [WWS+22]. Nevertheless, the input length limit of existing LLMs prevents them from
generalizing to real-world scenarios where the capability of processing long-form information beyond
a fix-sized session is critical, e.g., long horizontal planning.




To address the length limit issue, the most straightforward method is to simply scale up the input context length. For instance, GPT-3 [BMR+20] increases the input length from 1k of GPT-2 [RWC+19]
to 2k tokens for capturing better long-range dependencies. However, this approach typically incurs
computation-intensive training from scratch and the in-context dense attention is still heavily constrained by the quadratic computation complexity of Transformer self-attention [VSP+17]. Another
recent line of work [BPC20, ZGD+20] instead focuses on developing in-context sparse attention to
avoid the quadratic cost of self-attention, which still largely requires training from scratch. In contrast,
the prominent work, Memorizing Transformer (MemTRM) [WRHS22], approximates in-context
sparse attention via dense attention over both in-context tokens and memorized tokens retrieved from
a non-differentiable memory for Transformers. Thus, MemTRM scales up the resulting language
model to handle up to 65k tokens and achieves substantial perplexity gains in modeling full-length
books or long papers. However, MemTRM faces the memory staleness challenge during training
due to its coupled memory design, which uses a single model for encoding memory and fusing
memory for language modeling. In other words, as the model parameters are updated, cached older
representations in memory may have distributional shifts from those from the latest model, thereby
limiting the effectiveness of the memory augmentation.


In this paper, we propose a framework for Language Models Augmented with Long-Term Memory
(LONGMEM), which enables language models to cache long-form previous context or knowledge
into the non-differentiable memory bank, and further take advantage of them via a decoupled memory
module to address the memory staleness problem. To achieve decoupled memory, we design a
novel residual side-network (SideNet). Paired attention keys and values of the previous context are
extracted using a frozen backbone LLM into the memory bank. In the memory-augmented layer
of the SideNet, the generated attention query of the current input is used to retrieve cached (keys,
values) of previous contexts from the memory, and the corresponding memory augmentations are
then fused into learned hidden states via a joint-attention mechanism. Furthermore, newly designed
cross-network residual connections between the SideNet and the frozen backbone LLM enable better
knowledge transfer from the pretrained backbone LLM. By continually training the residual SideNet
to retrieve and fuse memory-augmented long-context, the pre-trained LLM can be adapted to leverage
long-contextual memory for improved modeling. The detailed memory cache, retrieval and fusion
process is illustrated in Figure 1.




Our decoupled memory design leads to two main benefits. First, our proposed architecture decouples
the process of encoding previous inputs into memory and the process of memory retrieval and fusion
by decoupled frozen backbone LLM and SideNet. In this way, the backbone LLM only works as
the long-context knowledge encoder, while the residual SideNet works as the memory retriever and
reader, which effectively resolves the issue of memory staleness. Second, directly adapting the entire
LLM with memory augmentations is computationally inefficient, and also suffers from catastrophic
forgetting. As the backbone LLM is frozen during the efficient memory-augmented adaptation stage,
LONGMEM can not only tap into the pretrained knowledge but also avoid catastrophic forgetting.



LONGMEM is capable of taking various types of long-form text and knowledge into the memory bank
based on downstream tasks. Here, we consider two representative cases, language modeling with
full-length book contexts, and memory-augmented in-context learning with thousands of task-relevant
demonstration examples. Specifically, we evaluate the effectiveness of the proposed LONGMEM
on various long-text language modeling, and memory-augmented in-context learning for language
understanding. Experimental results demonstrate that our model consistently outperforms the strong
baselines in terms of long-text modeling and in-context learning abilities. Our method substantially
improves LLM’s long-context language modeling capabilities by -1.38∼-1.62 perplexity over different length splits of Gutenberg-2022 corpus. Remarkably, our model achieves the state-of-the-art
performance of 40.5% identification accuracy on ChapterBreak, a challenging long-context modeling
benchmark, significantly surpassing existing strong x-former baselines. Lastly, with 2k demonstration
examples in memory, LONGMEM shows pronounced in-context learning improvements on popular
NLU tasks, compared with MemTRM and non-memory-augmented baselines.





