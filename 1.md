Self-supervised pre-trained language models (LMs)
like ELMo (Peters et al., 2018) and BERT (Devlin
et al., 2019) learn powerful contextualized representations. With task-specific modules and finetuning, they have achieved state-of-the-art results
on a wide range of natural language processing
tasks. Nevertheless, open questions remain about
what these models have learned and improvements
can be made along several directions. One such
direction is, when downstream task performance
depends on relational knowledge – the kind modeled by knowledge graphs 1
(KGs) – directly finetuning a pre-trained LM often yields sub-optimal
results, even though some works (Petroni et al.,
2019; Davison et al., 2019) show pre-trained LMs
have been partially equipped with such knowledge.


To address this shortcoming, several recent
works attempt to integrate KGs into pre-trained
LMs. These approaches can be coarsely categorized into two classes, as shown in Figure 1. The
first line of methods retrieves a KG subgraph (Liu
et al., 2019a; Lin et al., 2019; Lv et al., 2019)
and/or pre-trained graph embeddings (Zhang et al.,
2019b; Peters et al., 2019) via entity linking during
both training and inference on downstream tasks.
While these methods inject domain-specific knowledge directly into language representations, they
rely heavily on the performance of the linking algorithm and/or the quality of graph embeddings.
Graph embeddings, to be tractable over large-scale
KGs, are often learned using shallow models (e.g.,
TransE (Bordes et al., 2013), TuckER (Balazevic
et al., 2019) and ConvE (Dettmers et al., 2018))
with limited expressive power. Besides, the linking
and retrieval invoked during both finetuning and
inference are costly, hence limiting these methods’
practicality.


The second class of methods (Bosselut et al.,
2019; Malaviya et al., 2019; Yao et al., 2019) uses
contextualized representations from pre-trained
LMs to enrich graph embeddings and thus alleviates graph sparsity issues. This is especially
helpful in the case of commonsense KGs (e.g.,
ConceptNet (Speer et al., 2017)) that consist of
non-canonicalized text and hence suffer from severe sparsity (Malaviya et al., 2019). Specifically,
these methods usually feed concatenated triples
(e.g., [HEAD, Relation, TAIL]) into LMs for training or finetuning. The drawback is that focusing
on knowledge base completion tends to over-adapt
the models to this specific task, which comes at the
cost of generalization to text-based tasks, e.g., QA.



In this work, we equip masked language models (MLMs), e.g., BERT (Devlin et al., 2019),
with structured knowledge via self-supervised pretraining on raw text. Compared to the first class, we
expose LMs to structured information only during
pre-training, thus circumventing costly knowledge
retrieval and integration in both finetuning and inference. Also the dependency on the performance
of linking algorithm is greatly reduced. Compared
to the second class, we learn from free-form text
through MLMs rather than triples, which fosters
generalization on other downstream tasks.




Specifically, given a corpus of raw text and a
KG, two KG-guided self-supervision tasks are formulated to inject structured knowledge into MLMs.
First, taking inspiration from Baidu-ERNIE (Sun
et al., 2019a), we reformulate the masked language
modeling objective to an entity-level masking strategy, where entities are identified by linking their
text mentions to either concepts in a commonsense
KG or named entities in an ontological KG (Bollacker et al., 2008). The role of KG here is to provide a “vocabulary” of entities to be masked. To
further exploit implicit relational information underlying raw text, we design a KG-guided masking
scheme that selects informative entities by considering both document frequency and mutual reachability of the entities detected in the text. In addition to the new entity-level MLM task above,
a novel distractor-suppressed ranking task is proposed. Negative entity samples are derived from
the KG and used as distractors for the masked entities to make the learning more effective.



Note that our approach never observes the KG
directly, through triples or other forms. Rather,
the KG plays a guiding role in the proposed selfsupervised tasks. Its guidance helps the model
exploit the corpus more effectively as verified in
the experiments. If a downstream task can benefit
from explicit exposure to KG, a method by Davison
et al. (2019) can be used to transform KG triples
into natural grammatical texts for our model.

We evaluate our method on five benchmarks (including question answering and knowledge base
completion) and one zero-shot testing. Results
show our method achieves state-of-the-art or competitive performance on all benchmarks.



