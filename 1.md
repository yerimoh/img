4.3 Named Entity Recognition (NER)


We compare two state-of-the-art models for NER,
(a) a sequence tagger and (b) a dependency parser.
For the sequence tagger, we encode the NE labels
using the nested BILOU scheme (Alex et al., 2007),
which leverages a label set of combined types constructed from the training set for nested NEs. As
there are only very few cases (about 0.65% of all
NE annotations) where a token receives more than
three stacked NE labels, in order to avoid sparsity
issues, we consider only the “bottom” three layers of stacked entities. We feed the contextualized
embeddings of the last transformer layer of the respective first wordpiece token of each “real” token
into a linear layer and then use a CRF (Lafferty
et al., 2001) to optimize predictions for the entire
sequence.
Modeling NER as a dependency parsing task
(Yu et al., 2020) can easily account for nested
NEs. The main idea is to predict edges reaching from the end token of an NE to its start token as depicted in Figure 2. We adapt the STEPS
parsing pipeline (Grünewald et al., 2021a) to the
task. There are three combinations of tags in our
dataset that occasionally cover the exact same span
and that occur more than 20 times: VALUE+NUM,
VALUE+RANGE and MAT+FORM. We hence introduce the above combined labels. For any other
infrequent conflicting labels, we do not add extra
tags, i.e., the model can never catch these cases.
We decide on this slight restriction of the model
capabilities in order to avoid sparsity issues. In the
evaluation, we do not filter for these cases but of
course use all nested NEs as annotated.

4.4 Relation Extraction

Given an input sentence along with all named entities within it, as well as their types (either gold or
predicted depending on the experimental setting),
we predict which (if any) relation is present between them. We treat all relations in a single model
and predict all relations of a sentence simultaneously by modeling relation extraction as a graph
parsing task. Following Toshniwal et al. (2020), we
first create an embedding ei for the i
th NE in the
sentence by concatenating the token embeddings
of its first and last token (ei,START, ei,END). We also
concatenate a learned embedding for the NE’s label
(ei,LABEL): ei = ei,START ⊕ ei,END ⊕ ei,LABEL

Considering NEs as nodes in a graph, we use a
biaffine classifier architecture (Dozat and Manning,
2017) using the implementation of Grünewald et al.
(2021a,b) to predict the relation between each pair.
The non-existence of a relation is encoded as simply another label (∅). For details on the parser
architecture, see Appendix B



