Language models define probability distributions
over sequences of tokens. Given such a sequence
x1,...,xn, the standard way to model its probability is via next-token prediction: p(x1,...,xn) =
n
i=1 p(xi|x<i), where x<i := x1,...,xi−1 is the
sequence of tokens preceding xi, also referred to
as its prefix. This autoregressive model is usually
implemented via a learned transformer network
(Vaswani et al., 2017) parameterized by the set of
parameters θ:



where the conditional probabilities are modeled by
employing a causal self-attention mask (Radford
et al., 2018). Notably, leading LMs such as GPT-2
(Radford et al., 2019), GPT-3 (Brown et al., 2020),
OPT (Zhang et al., 2022), or Jurassic-1 (Lieber
et al., 2021) follow this simple parameterization.



Retrieval augmented language models
(RALMs) add an operation that retrieves one or
more documents from an external corpus C, and
condition the above LM predictions on these documents. Specifically, for predicting xi, the retrieval
operation from C depends on its prefix: RC(x<i),
so the most general RALM decomposition is:
p(x1,...,xn) = n
i=1 p(xi|x<i, RC(x<i)). In
order to condition the LM generation on the
retrieved document, previous RALM approaches
used specialized architectures or algorithms
(see §2). Inspired by the success of In-Context
Learning (Brown et al., 2020; Dong et al., 2023),
In-Context RALM refers to the following specific,
simple method of concatenating the retrieved documents2 within the Transformer’s input prior
to the prefix (see Figure 1), which does not
involve altering the LM weights θ:



where [a; b] denotes the concatenation of strings a
and b


Since common Transformer-based LM implementations support limited length input sequences,
when the concatenation of the document and the
input sequence exceed this limit we remove tokens from the beginning of x until the overall input
length equals that allowed by the model. Because
our retrieved documents are passages of limited
length, we always have enough context left from
x (see §4.3).
