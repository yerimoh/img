Evaluating the performance of the language model
on MatSci-NLP requires determining if the text
generated by the decoder is valid and meaningful
in the context of a given task. To ensure consistency in evaluation, we apply a constrained decoding procedure consisting of two steps: 1) Filtering
out invalid answers through the predefined answer
schema shown in Figure 2 based on the structure
of the model’s output; 2) Match the model’s prediction with the most similar valid class given by the
annotation for the particular task. For example, if
for the NER task shown in Figure 1 the model’s predicted token is “BaCl2 2H2O materials”, it will be
matched with the NER label of “material”, which
is then used as the final prediction for computing
losses and evaluating performance. This approach
essentially reformulates each task as a classification
problem where the classes are provided based on
the labels from the tasks in MatSci-NLP. We then
apply a cross-entropy loss for model fine-tuning
based on the matched label from the model output.
The matching procedure simplifies the language
modeling challenge by not requiring an exact match
of the predicted tokens with the task labels. This in
turns leads to a more comprehensible signal in the
fine-tuning loss function.
