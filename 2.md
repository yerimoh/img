Despite its satisfactory segmentation performance,
it is challenging to use our CANINE-based segmentation model as a language model tokenizer
due to its vocabulary-less nature. Since this model
can segment words arbitrarily using our character classification framework, unseen words can be
split into subwords which have never been seen
by the LM. Thus, pre-training a language model
using this tokenizer would require allowing the
model to increase its vocabulary size without limit
while training and using an out-of-vocabulary token
when unseen tokens are encountered during inference. This would lead to a language model with
an exceedingly large vocabulary size (which would
increase the cost of pre-training significantly) and
potentially limited generalization ability to unseen
tokens.


To tackle this problem, we introduce a tokenizer
which uses the same left-to-right decoding algorithm used by BERT and PubMedBERT but replace
its vocabulary with one designed for biomedical
segmentation. In order to build a vocabulary which
covers important biomedical tokens, we leverage
the Unified Medical Language System, a medical
knowledge base which contains approximately 15
million medical concept phrases. As shown in Figure 1 (center), we extract all single words from
UMLS concept phrases and segment each of them
using the CANINE-based tokenizer. This produces
around 250,000 unique subwords which we further reduce to 55,580 by eliminating ones which
only appear once in the CANINE segmented set
of UMLS words. In order to avoid segmenting
standard English words in unintuitive ways due
to the higher proportion of biomedical subwords,
we augment our 55,580 biomedical subwords with
the original BERT vocabulary as seen in Figure 1
(right). After removing duplicate tokens, we are left
with a vocabulary of 80,181 tokens. This carefully
designed vocabulary enables our new tokenizer,
BioVocabBERT, to obtain a segmentation score of
48.5% on the SIGMORPHON biomedical subset
as seen in 3, outperforming the best scoring current wordpiece-based tokenizer PubMedBERT by
almost 30 points.
