Create a notebook called threelayer.ipynb.
Extend the fully-connected two layer perceptron shown in class for the regression
problem by one more layer to have two hidden layers.


For this, take a long look at the derivation of the backpropagation. As you can see,
the derivation of the internal derivatives is always the same, no matter the number
of hidden layers.


Armed with this knowledge, it should be easy to extend the different functions in
the notebook.


The create_model function should now of course receive a list of values for the
number of hidden_nodes in each layer - in addition, please extend the functionality,
so that I can pass the activation function type as a string already here:


def create_model(X, hidden_nodes, output_dim = 2,
activation_function = ’relu’)

forward, calculate_loss, backprop should of course be extended
accordingly to deal with the added number of layers and the activation function
choice.


Next, do a series of tests for the x^2+y^2+1 function using NO sgd, and NO
regularization, a “relu” function, comparing the “old” two-layer version with your
“new” three-layer version as follows using the same (meaningful) hyperparameters:


- take 8 neurons for the two-layer version and 4 + 4 neurons for the three-layer
version, and run each network 20 times, recording the loss and the number
of iterations it needs


- repeat this with 16 neurons for the two-layer version and 8+8 neurons for the
three-layer version and 20 runs each


- which network architecture converges “better” (earlier? lower error?). Plot
the results nicely in one graph for errors and in another for number of
iterations and comment on the results.
















---------------------






Create a notebook called multilayer.ipynb.

As you hopefully saw when extending the logic to a third layer, there may be a
pattern here on how to deal with hidden layers!


Keeping this in mind, change all functions such that they can accept an arbitrary
number of layers, but keep the overall call-logic and training loops the same - do
NOT use classes!


For this, you will need to play around with the dictionaries in create_model,
forward, backprop.


Try to find a good multilayer network for fitting the x^2+y^2+1 function using some
“good” hyperparameters and insert the loss plot into the notebook alongside
comments on your experiments.
