Following symbolic knowledge distillation (West
et al., 2021), we distill large quantities of highquality knowledge from very large, general foundation models (§2.1.1) – we call the resulting dataset
NOVATOMIC. One major difference from previous
knowledge graphs is that we allow an open relation set, in the form of queries rather than fixed
relation tokens. While commonsense knowledge
often takes a head, relation, tail format with a
fixed set of discrete relations (e.g. X buys a lottery ticket, xWant, to win.), we propose a context,
query, inference (CQI) format with natural language queries serving as open relations. We also
analyze unique properties of this distilled knowledge in §2.1.2.
