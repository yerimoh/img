Inference over Streams as an MDP Problem. 


We
consider stream processing scenarios that have as input a
fixed infinite stream X = ⟨x1, . . . , xt, . . .⟩ of user queries.
The t-th query xt is associated with a ground-truth label
yt ∈ Y , where Y is a label set. Our goal is to predict the
label for each xt using an N-level model cascade.
We formulate our problem using an episodic Markov decision process (MDP) (S, A, T , C). Here:
• S is a set of states. A state in the t-th episode is either
a pair ⟨xt, i⟩, where i ∈ {1, ..., N} indicates the current
cascade level, or a special terminal state exit that ends
the episode. The initial state of the t-th episode is ⟨xt, 1⟩.
For clarity, we abbreviate ⟨xt, i⟩ by st,i.
• A is a set of actions, consisting of:
– The label set Y , representing the potential predictions
if the cascade chooses to output at the current state. For
instance, in a binary classification task, Y = {0, 1}.
– A special action defer that activates the next level of
the cascade

• T (st,i, a) is a deterministic transition function, consisting
of transitions of the form:
– T (st,i, a) = exit for a ∈ Y .
– T (st,i, defer) = st,i+1.

• C(st,i, a) is a cost function defined as:
C(st,i, a) = (
L(a|yt) if a ∈ Y ,
µci+1 if a =defer.
Here, L(a|yt) is a prediction loss that measures the accuracy of the cascade’s prediction. ci+1 represents the
penalty we pay for a deferral — intuitively, this penalty
captures the computational overheads of going one level
deeper into the cascade. The adjustable constant µ guides
the trade-off between computational cost and accuracy.


Online Cascade Learning. 


We now formulate online cascade learning as the problem of solving the above episodic
MDP. Let a policy π be a stochastic map from states to actions. We use π(st,i, defer) to represent the probability
that π chooses to defer in state st,i, and π(st,i, y) to represent the probability that π chooses y ∈ Y , conditioned on
no deferral having occurred.
The probability of a policy π entering state st,i in episode
t is denoted as p
st,i
π . For i > 1, st,i can be reached if and
only if the policy chooses the defer action in all preceding
states st,1, ..., st,i−1 within the current episode. Thus,
p
st,1
π = 1, pst,i
π =
i
Y−1
j=1
π(st,j , defer).
Then, the cost of executing π over T episodes is computed
by summing over all the episodes and cascade levels:
J(π, T) = X
T
t=1 "X
N
i=1
p
st,i
π Cπ(st,i)
#
(1)
Here, Cπ(st,i) is the expected, immediate cost of applying
policy π at state st,i. This is computed as:
Cπ(st,i) = π(st,i, defer) · µci+1
+ (1 − π(st,i, defer)) ·
X
y∈Y
π(st,i, y) · L(y|yt).
After having seen the first T queries in the input stream
X, our learning goal is to find a policy that minimizes
J(π, T). When T is clear from the context, we often abbreviate J(π, T) by J(π).










Policy Representations. 


We represent policies in a factorized way using a set of classification models ⟨m1, . . . , mN ⟩
that constitute the different levels of the cascade, and a set of
deferral functions ⟨f1, . . . , fN−1⟩ that decide whether the
current level can perform a high-confidence classification
or to defer. We assume each mi
to produce a vector of probabilities, with one probability for each label, and each fi
to
produce a probability of deferral. Then the overall policy
has the form:
π(st,i, defer) = fi(mi(xt)),
π(st,i, y) = (mi(xt)) [y] for y ∈ Y.
We assume the two parameterized function representations
for each level i: the classification model mi and the deferral
function fi
, are both characterized by a crucial property:
they guarantee uniform computational costs for evaluating
the functions, regardless of the specific function parameters
and function inputs. This means that for any instantiation
of the parameters in mi and fi
, the inference costs remain
constant, irrespective of the specific input query. This assumption is reasonable in the practical scenarios we target.
For example, the inference cost of a BERT-base model is
approximately the same, no matter how it is parameterized.
This uniform cost assumption also underpins the use of fixed
cost penalties ci
in our MDPs.
