All the findings above suggest that biomedical language model pre-training yields entity representations which are fairly robust to tokenization failures.
However, it is important to note that the distribution of rare vs. frequent entities in these small and
medium scale datasets will be naturally skewed
towards frequent entities if not intentionally manip
ulated. Therefore, we design an experiment which
explores whether the quality of representations
in BioVocabBERT and our PubMedBERT replica
varies with respect to word frequency. In this experiment, we obtain 10,000 instances of words from
the pre-training corpus in each of the frequency
bins listed in Table 2. We encode the sentence in
which each instance is found and mask the word of
interest. We report the percentage of words which
are predicted correctly using the masked language
modeling head’s prediction in each bin



We note that both models perform very similarly across frequency bins until the two bins with
the largest frequencies. Our BioVocabBERT model
obtains a somewhat significant advantage in the second highest 500-5,000 frequency bin which then
inverts to a similarly significant drop in the category with the most frequent words. This trade-off
is likely due to many high frequency words having a single token in the PubMedBERT vocabulary, which leads the model to have a natural bias
towards predicting these words. In medium frequency words, PubMedBERT’s bias towards high
frequency words is likely detrimental and BioVocabBERT is able to easily outperform it. This tradeoff appears to be small enough to have little effect
on downstream performance for these models but
we leave exploring its effect for future work.
