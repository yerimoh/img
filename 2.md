Knowledge Generation

Pretrained language
models demonstrated the ability to carry implicit
knowledge (Petroni et al., 2019; Dhingra et al.,
2022). These large language models are prompted
for generating new knowledge to perform downstream tasks such as text classification (Shin et al.,
2020; Puri and Catanzaro, 2019), commonsense
reasoning (Liu et al., 2022b; Trinh and Le, 2018;
Davison et al., 2019). We take inspiration from
commonsense LMs, designed for query commonsense knowledge, such as COMET (Bosselut et al.,
2019) and COMET-2020 (Hwang et al., 2021).
Domain specific LMs are also used for knowledge graph completion in specialized domains like
biomedicine (Nadkarni et al., 2021). Liu et al.
(2022a) use dataset cartography to prime the model
with challenging examples and enable it to generate
more examples with such patterns.



Knowledge Distillation 

As the process of manually creating datasets can be costly and complex,
prior studies have explored the realm of automated
data generation. These prior works mainly focused
on extractive approaches, e.g. syntactic parsing
(Zhang et al., 2020a) or pattern matching (Li et al.,
2020) from unstructured text (Lehmann et al., 2015;
Buck et al., 2014).

West et al. (2021) proposed filtering out low quality data using a critic model for symbolic knowledge distillation from larger models. Following
this, several works effectively improved upon this
for iterative distillation (Sclar et al., 2022; Bhagavatula et al., 2023), self-chat with feedback and
conversations with ChatGPT (Xu et al., 2023; Geng
et al., 2023; Chiang et al., 2023). SODA (Kim et al.,
2023) contextualized social commonsense knowledge from a knowledge graph to distill dialogues
from InstructGPT. Sclar et al. (2022) established
filters based on length, fidelity, and information bottleneck for distilling reference-free summarization
determining the effectiveness of designing filters
for selecting data for the following iteration. Recently, (Jung et al., 2023) proposed a framework
to learn a high-quality model from a low-quality
teacher model to distill a good dataset by summarizing and paraphrasing.
