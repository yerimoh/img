In order to communicate complex concepts precisely and efficiently, biomedical terminology has
been designed by researchers and medical professionals by combining existing meaningful morphemes to create new concepts. Many biomedical
terms use general rules to combine meaningful morphemes taken from Greek and Latin (Banay, 1948).
For example, these morphemes often have vowels
that can be omitted such as the ‘-o-’ in ‘nephro’
from Table 1. This ‘-o-’ acts as a joint-stem to
connect two consonantal roots (e.g. ‘nephr-’ + ‘-
o-’ + ‘-pathy’ = ‘nephropathy’), but the ‘-o-’ is
often dropped when connecting to a vowel-stem
(e.g. ‘nephr-’ + ‘-ectomy’ = ‘nephrectomy’, instead
of ‘nephr-o-ectomy’). Students in biomedical fields
often learn the meaning of these elements as well
as the word formation rules to be able to infer the
meaning of unfamiliar words and recall complex
terms more easily.


Even though the agglutinating nature of biomedical terminology is well known, none of the existing pre-trained language models consider this
information explicitly when building their tokenizers. As shown in Table 1, frequent words such
as ‘nephropathy’ and ‘nephrectomy’ are tokenized
by BERT (Devlin et al., 2019) into meaningless
subwords (‘ne-ph-rop-athy’ and ‘ne-ph-re-ct-omy’)
while remaining as whole words for PubMedBERT
(Gu et al., 2021). For more infrequent but still important medical terms like ‘nephroblastomas’ and
‘nephrocalcinosis’, PubMedBERT encodes them as
‘nephr-oblastoma’ and ‘nephr-ocalcin-osis’. We argue that there are more meaningful and efficient ways to tokenize both frequent and infrequent
medical terms using meaningful morphemes like
‘nephr(o)’ (of a kidney), ‘-pathy/-(o)sis’ (disease),
‘-ectomy’ (surgical procedure), ‘calcin’ (calcification) and ‘blastoma’ (type of cancer) which could
help models transfer signal directly into infrequent
and even out-of-vocabulary terms.



In this work, we first leverage large-scale morpheme segmentation datasets to more rigorously
evaluate current tokenization methods both quantitative and qualitatively. Using the annotated morpheme segmentation dataset from the SIGMORPHON 2022 Shared Task (Batsuren et al., 2022),
we are able to determine that current tokenizers
such as BERT and the more biomedically relevant
PubMedBERT are very poorly aligned with human
judgments on morpheme segmentation, even when
evaluating on biomedical terminology specifically.


Given that, although the PubMedBERT tokenizer exhibits low performance on biomedical
morpheme segmentation, it shows some improvement over BERT’s tokenizer due to its use of
biomedical corpus statistics, we hypothesize that
using a tokenizer that aligns more strongly with
standard biomedical terminology construction for
pre-training could achieve improved performance
in downstream tasks. In order to verify this hypothesis, we create a new tokenizer, BioVocabBERT,
which uses a vocabulary derived from combining
a fine-tuned morpheme segmentation model with
biomedical domain-knowledge from the Unified
Medical Language System (UMLS) (Bodenreider,
2004), a large scale biomedical knowledge base.
Subsequently, we leverage BioVocabBERT, which
greatly outperforms the PubMedBERT tokenizer
on biomedical morpheme segementation, to pretrain a biomedical language model by the same
name and compare its performance with a replicated PubMedBERT model (to control for any potential differences in the pre-training process) on
two downstream tasks: named entity recognition
(NER) and entity linking.






Surprisingly, we find that the performance of
our BioVocabBERT model is remarkably similar
to the one obtained by our PubMedBERT replica
throughout most datasets tested in fully supervised
NER, low-resource NER and zero-shot entity linking. Small improvements arise in low-resource
NER and zero-shot entity linking but results are inconsistent across datasets. Additionally, we exam
ine the model’s robustness to segmentation failures
in a small scale case-study which suggests that even
if the model’s word embeddings are biased by tokenization errors, the model’s parameters are able to
overcome such failures quite successfully. Finally,
we measure our models’ language modeling accuracy by word frequencies and find a small word
frequency trade-off whose exploration we leave for
future work. Given these findings, we conclude that
biomedical language model pre-training is quite robust to tokenization decisions which are not well
aligned with human judgments, even when dealing
with highly agglutinating biomedical terminology
















