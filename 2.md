As no other resource currently has examples of high-quality commonsense inferences in our proposed open format, we develop a set of few-shot examples of
10 contexts (either handwritten or selected from
ATOMIC10X or from ROCStories (Mostafazadeh
et al., 2016)) with 10 handwritten commonsense
query/inference pairs for each (see Appendix B.2
for more details). These query/inference pairs
cover a broad swathe of knowledge, including consequences, explanations, reactions, attributes, counterfactuals, etc



For each context in NOVATOMIC generated in
the previous step, we randomly select and permute
n โผ Uniform(1, 10) of the few-shot examples to
provide in context after the instructions and then
task the model with generating 10 query/inference
pairs for the new context. The rationale for this
random selection and permutation of the few-shot
examples is to mitigate the potential overfitting
to a specific ordering or selection or ordering of
handwritten examples. To try to support the use
case where a desired commonsense query is not
known in advance, e.g. when a user simply want
general knowledge for a given context, we also
generated half of the commonsense hypotheses
without generating a query first (prompt in B.2).
At training time (ยง2.3), we input a NULL value
for the query field. We generated all query/inference pairs using default decoding arguments
with gpt-3.5-turbo-0301 for a total cost of USD
$337.16.
