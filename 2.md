Recent work on domain-specific language models has demonstrated fairly conclusively that using domain-specific data for pre-training significantly improves language model performance on
in-domain downstream tasks. Many different such
strategies have been proposed with varying degrees
of in-domain vs out-of-domain pre-training data in
fields such as biomedicine (Peng et al., 2020; Lee
et al., 2019; Gu et al., 2021; El Boukkouri et al.,
2022), finance (Wu et al., 2023), law (Chalkidis
et al., 2020), scientific research (Maheshwari et al.,
2021), clinical practice (Alsentzer et al., 2019) and
social media (DeLucia et al., 2022). For biomedical language models specifically, most work agrees
that pre-training from scratch using an in-domain
corpus, as done by Gu et al. (2021), leads to small
but measurable performance improvements over
other pre-training strategies.




Apart from introducing pre-training from
scratch, Gu et al. (2021) demonstrated the limitations of general domain tokenization for domainspecific pre-training by showing downstream improvements obtained from using a domain-specific
tokenizer, created by standard tokenizer building
algorithms such as WordPiece (Schuster and Nakajima, 2012) and BPE (Gage, 1994) on an in-domain
corpus. In-domain tokenizers have also been shown
to improve performance in other domains such as
law (Chalkidis et al., 2020) and more specific ones
like cancer (Zhou et al., 2022). As a result, most
recent biomedical LMs use tokenizers created from
in-domain corpora statistics (Yasunaga et al., 2022;
Luo et al., 2022).



2.2 Limits of Unsupervised Tokenization

Even though domain-specific tokenizers have become widely used for biomedical LMs, they are
still constructed using mainly unsupervised algorithms which leverage information theoretic metrics from large-scale corpora to create subword vocabularies. However, as reported in the SIGMORPHON 2022 Shared Task for morpheme segmentation (Batsuren et al., 2022), these methods align
little with morphological human judgments. Hofmann et al. (2021) explore how poor segmentation
affects performance by injecting rule-based derivational morphology information into the tokenization process and showing improvements in word
classification tasks, especially for low-frequency
words. As far as we know, our work is one of the
first to perform a similar morpheme segmentation
analysis on biomedical tokenizers, even though
biomedical terminology is highly agglutinating by
design and should benefit from such analysis.



Furthermore, Hofmann et al. (2020) shows that
introducing derivational morphology signal into
BERT via fine-tuning improves its derivation generation capabilities, suggesting that performance of
language models could be improved by adding morphologically relevant signal into their pre-training.
Nevertheless, not much work apart from our current study has explored how introducing such signals could affect the pre-training process directly,
especially not in biomedical language models.
