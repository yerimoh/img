We evaluate our LLM cascade approaches on six datasets, covering (1) mathematical reasoning,
including GSM8k (Cobbe et al., 2021), ASDIV (Ling et al., 2017), and TabMWP (Lu et al., 2023);
(2) symbolic reasoning from BIG-Bench Hard (bench authors, 2023), including DATE and Navigate; and (3) causal reasoning, including CREPE (Zhang et al., 2023). In our pipeline, we leverage
the GPT-3.5-turbo (4k context) as the weaker LLM and the GPT-4 (8k context) with CoT selfconsistency (Wang et al., 2023, SC) as the stronger LLM. Throughout our experiments, we set the
number of task demonstrations as M = 8. We use the same demonstration examples as prior
work (Chen et al., 2022; Wei et al., 2022). When additional task demonstrations are needed (e.g.,
for 2D approaches), we randomly sample examples from the training data and manually annotate
them with thought representations. We set the number of sampling paths as K = 20 for GPT-3.5-
turbo and K = 3 for GPT-4. The sampling temperature by default is 0.4 for both LLMs. The
metrics we use are the task accuracy and the relative cost compared with the cost of GPT-4 with
CoT SC (denoted as GPT-4-CoT-SC). A lower relative cost and higher accuracy indicate better performance. We also compared our approaches with baselines using only the weaker LLM
(i.e., GPT-3.5-CoT-SC, GPT-3.5-PoT-SC) or only the strong LLM in different ways (i.e.,
GPT-4-CoT-Greedy, GPT-4-PoT-Greedy, GPT-4-CoT-SC, GPT-4-PoT-SC). Our experimental details can be found in Appendix B and reproducible prompts in Appendix L.




3.2 MAIN RESULTS




Figure 3 illustrates the performance of our proposed approaches. For Vote-based approaches, we
draw curves by changing the pre-defined threshold τ varying from 0.4 to 1. A high value of threshold
signifies a more rigorous criterion for trusting the answers from the weaker LLM, making more
examples transferred to the stronger LLM. Our observations are as follows:





Our pipeline achieves comparable task performance with significantly reduced costs.


On average, all of our cascade variants (Vote or Verify) demonstrate significant cost efficiency. In
particular, as shown in the average plot, the four MoT variants achieve comparable task performance (∼0.929 accuracy) to GPT-4-CoT-SC (0.931) while demanding only 40% of its cost. On
CREPE, MoT variants even outperform GPT-4-CoT-SC (0.885 vs. 0.871) at 47% of its cost. Our
approaches based on CoT and PoT also exhibit the capability to save costs while maintaining the
overall task performance. For example, CoT-2D-Vote achieved 0.924 task accuracy on average
but demanded only 57% relative cost. These observations suggest the effectiveness of our cascade
decision maker via checking the answer consistency of the weaker LLM.




Sampling from diverse prompt settings helps cascade decision-making. 



Our results show
that variants involving diverse sources of sampling, such as CoT/PoT-2D-Vote and
MoT-1D/2D-Vote, can more precisely distinguish between easy and hard reasoning questions,
compared with their counterparts sampling from single sources, i.e., CoT/PoT-1D-Vote. For
example, between CoT-2D-Vote and CoT-1D-Vote, the former outperforms the latter by 1.4%
absolute accuracy under the same relative cost of 0.4 on average






Mixing thought representations is particularly effective. 


Furthermore, we find that mixing the
two thought representations (i.e., MoT-1D/2D-Vote) outperforms decision-making using either
of them (i.e., CoT-1D/2D-vote and PoT-1D/2D-vote). This is illustrated by the gap in the
average plot and is consistent on most datasets except DATE, where many test questions are very
similar to the demonstration examples. Intuitively, this is because different thought representations
can bring in more diverse “opinions” of the weaker LLM on the same input question, resembling
how a group of experts with diverse perspectives could contribute to more effective results in collaborative work. It can also be viewed as “ensembling” LLMs, which utilizes the intuition that variants
of the same model typically share few mistakes (Rokach, 2010). We provide a further investigation
of this effect in Section 3.3. We also note that when using MoT, no obvious difference is perceived
between using one set (i.e., MoT-1D-Vote) or two sets (i.e., MoT-2D-Vote) of task demonstrations. This result reveals that tuning the thought representations is more helpful for measuring an
LLM’s (un)certainty on its answer than tuning the task demonstrations.




Increasing the threshold yields marginal benefits for MoT-1D/2D-Vote. 



As costs increase,
the curves of MoT-1D/2D-Vote flatten out, showing that pushing the threshold to exceedingly
high is unnecessary. This is because even for an easy question, the weaker LLM may still have hallucinations in a small set of answers. Setting the threshold too high can lead to the decision-making
being influenced by them. It may result in easy questions being sent to the stronger LLM incorrectly,
thus driving up the overall cost. In our practice, a threshold that can balance cost and accuracy typically falls between 0.5 and 0.6. When it comes to MoT-1D/2D-Verify, we can strike a balance
between cost and accuracy as the verification method can tolerate a few hallucinations.


