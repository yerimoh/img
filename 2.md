Text Preprocessing. The battery research papers were
retrieved using web-scraping tools from ChemDataExtractor22,23
and python HTTP client library ”requests”. More
details of article retrieval have been provided by Huang and
Cole.21 The literature data were initially downloaded in
HTML/XML format and saved in text format, in order to be
easily processed in the text-processing step. Then, a subword
tokenizer, WordPiece tokenizer,25 was used to decompose rare
words into smaller meaningful subwords. An example of a
subword tokenizer is as follows: a single word ”solvation” is
split into three words: so, ##l, and ##vation. In this way, the
vocabulary size can be reduced to a reasonable level in order to
improve the computational efficiency. The vocabulary files of
BatteryBERT and BatterySciBERT are the same as the original
BERT and SciBERT vocabulary, respectively; while the size of
both vocabularies is the same as the original model. For
BatteryOnlyBERT, we trained our own WordPiece tokenizer
from the battery-paper data sets. Figure 3 shows the
comparison of vocabularies of the three BERT models. The
resulting BatteryVocab file of BatteryOnlyBERT shares 40.4%
of the same words as the vocabulary file of the original BERT
model, while around 60% of words in BatteryVocab are brand
new and they are more about chemistry or batteries than the
general popular English words that were used by the original
BERT model. As shown in Figure 2a, the most common words
in the word cloud picture of the battery corpus, such as
electrode, energy, and batteries, demonstrate that the focus of
this vocabulary file is in the domain of batteries. This also
reflects that the text about battery research is a central part of
the corpus that further trains the BatteryBERT and
BatterySciBERT models.
