The word tokenizer has been designed to broadly match the
Penn Treebank policy, with some modifications to better
handle chemistry text. Tokens are split on all whitespace and
most punctuation characters, with exceptions for brackets,
colons, and other symbols in certain situations to preserve
entities such as chemical names as a single token. Additionally,
care is taken to consistently split units and mathematical
symbols from numeric values, regardless of whether the source
text contains a space between them.


Text normalization is an important step that removes
commonly occurring inconsistencies that have a detrimental
impact on the performance of machine learning and dictionary
methods, and add unnecessary complexity to parsing rules. In
contrast to other systems that normalize text prior to
tokenization, our tokenizer is designed to operate on any
unicode text input, and normalization is subsequently
performed on the text content of each individual token. The
advantage of this approach is that each token can retain a
pointer to its exact start and end position within the source text,
even if normalization then changes the length of tokens.
Therefore, the original token text can always be recovered, and
any information derived about a token can be easily annotated
back onto the original document.


As part of the normalization, unicode characters with similar
appearance that are often used interchangeably are standardized,
all nonprinting control characters are removed, and
alternative chemical spellings are unified.
