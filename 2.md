5.1. Overall Performance and Cost Trade-offs


IMDB. 

The results presented in Table 1, Figure 3 and 4
demonstrate that our proposed online cascade learning system can consistently achieve higher accuracies compared to
knowledge distillation and online ensemble learning baselines on the IMDB dataset across different cost budgets,
regardless of whether using GPT-3.5 Turbo or Llama 2 70B
Chat in the cascade. Notably, Figure 4 highlights our system’s ability to closely rival the performance of Llama 2
70B Chat while achieving a 60% reduction in inference
costs (i.e., calling LLM ∼5200 times in processing a total of
12500 queries). This effectively demonstrates the system’s
efficiency in balancing cost with performance.


HateSpeech. 

The results on the HateSpeech dataset further
reveal the strengths of our online cascade learning system
in handling datasets with significant class imbalance. Most
models may face a trade-off between accuracy and recall
due to the imbalanced nature of HateSpeech. However,
the accuracy-cost and recall-cost trade-offs, respectively
depicted in the upper and lower subplots of Figure 3 and
4, demonstrate that our system effectively improves recall
with minimal impact on accuracy as the cost budget increases. Although the recall rate of online cascade learning
is marginally lower than the baselines under certain budgets,
it can achieve a better balance between recall and precision, as evidenced by its consistently higher F1 scores in
Appendix Figure 10. In particular, as demonstrated in Table
1 where Llama 2 70B Chat is the LLM and the cost budget
N = 4900, our system even outperforms the LLM with a
similar recall (ours: 82.03% vs. LLM: 82.19%) and a better
accuracy (78.32% vs. 77.81%), underlining its effectiveness
in handling imbalanced data streams.



ISEAR. 

On the ISEAR benchmark, our online cascade
learning system also effectively balances cost and accuracy
in complex multi-class classification. As indicated in Figure
3 and 4, the system’s performance gradually aligns with that
of GPT-3.5 Turbo and even surpasses Llama 2 70B Chat
as the cost budget increases. This success underscores the
advantages of smaller models in adapting to complex classifications by learning from the LLM annotations, enabling
them to potentially outshine zero-shot LLMs. Moreover, the
notable performance gap between online ensemble learning
and online cascade learning also confirms the benefit of
co-optimizing model learning with deferral policy learning
for optimal cost-performance equilibrium.



FEVER. 


FEVER is a significantly more complex dataset
compared to the previous benchmarks. It demands models
to reason over the statements and validate their factuality
based on parametric knowledge. Therefore, small models
of limited capacities, such as logistic regression, struggle to
perform effectively on FEVER even after several iterations
of update, as evident in Table 1 where distilled LR can
perform only slightly better than random guess (i.e., 50%).
Recognizing these limitations, our online cascade learning
system smartly adapts by prioritizing more capable models,
such as BERT-base and the LLM, for processing most of
the queries, leading to a favorable accuracy-cost trade-off.
Remarkably, when using Llama 2 70B Chat as the LLM with
a cost budget of N = 2800, our system slightly outperforms
the LLM in accuracy (77.73% vs. 77.15%), showcasing the
system’s proficiency at navigating intricate reasoning tasks
with enhanced cost-efficiency


