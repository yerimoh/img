Confidence Calibration. A reliable deferral rule is crucial for a cascade system to determine whether to invoke the
next model (i.e., to defer) or to output the predictions. Currently, most existing works make deferral decisions based
on a confidence score, typically measured either by the maximum predictive probability across all classes (Wang et al.,
2022; Varshney & Baral, 2022), or the Shannon entropy of
the predictive distribution (Stogiannidis et al., 2023).
However, for online cascade learning, where the model capabilities are dynamically updated, and the annotations from
LLMs may be noisy, confidence-based deferral rules have
been shown to be inadequate (Jitkrittum et al., 2023). To
aid the learning of deferral functions (i.e., f1, ..., fN−1),
we adopt a post-hoc approach to calibrate the confidence
estimate of a certain model’s prediction mi(xt). It is implemented using a multi-layer perceptron (MLP) that takes the
corresponding model’s predictive probabilities as input, and
the functions can be updated with the following objective:
min
π
′
i
:R|Y |→R
X
xt∈X
N
X−1
i=1
L(fi(mi(xt)), zi), (5)
where zi = 1[arg max mi(xt) ̸= y
∗
t
] (i.e., zi = 1 if model
mi’s prediction is not equal to the annotation y
∗
t
, otherwise
zi = 0) and L is the mean-squared error loss function.
Since we treat the expert LLM predictions (i.e.,
arg max(mN (xt))) as the ground truth labels yt, calibration is only performed on those input queries where the
expert LLM is invoked. During the calibration, the learning of fi would only optimize the parameters of the MLP,
not the models mi
in the cascade. During inference time,
the post-hoc deferral function would choose to defer if
fi(mi(xt)) > 0.5. Otherwise, it would output the prediction of the current model.
