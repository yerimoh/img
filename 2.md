For tokenization, BioBERT uses WordPiece tokenization (Wu
et al., 2016), which mitigates the out-of-vocabulary issue. With
WordPiece tokenization, any new words can be represented by frequent subwords (e.g. Immunoglobulin Â¼> I ##mm ##uno ##g ##lo
##bul ##in). We found that using cased vocabulary (not lowercasing) results in slightly better performances in downstream tasks.
Although we could have constructed new WordPiece vocabulary
based on biomedical corpora, we used the original vocabulary of
BERTBASE for the following reasons: (i) compatibility of BioBERT
with BERT, which allows BERT pre-trained on general domain corpora to be re-used, and makes it easier to interchangeably use existing models based on BERT and BioBERT and (ii) any new words
may still be represented and fine-tuned for the biomedical domain
using the original WordPiece vocabulary of BERT.
