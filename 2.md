Here, we describe our LSKD pipeline to distill visual commonsense from a LLM. Prior works have
explored powerful LLM as the teacher model (GPT-3, ChatGPT) to apply knowledge distillation for
language-only reasoning tasks [58, 33, 3]. Multimodal inputs offer additional challenges in grounding
regions to relevant texts. Our work addresses this challenge by automatically generating reliable and
diverse knowledge statements for multimodal input, to further reason about regions within an image.




Figure 1 shows the overall framework of LSKD2. To learn from the LLM as our teacher model, we
verbalize the image into a set of dense text statements generated by global descriptors that provide
relevant, general overall semantics of the image, and local descriptors that talk about specific regions
in the image. We then pass these automatically generated descriptions to LLM and prompt to mine
localized, commonsense statements about the image at scale (See the Appendix for the exact prompt).




As LLMs comprehend multimodal input only through machine-generated image-to-text verbalization,
they are prone to hallucination and generation of inconsistent statements about the image. For
instance, an incorrect verbalizer output, as in Figure 1, might cause the LLM to produce visually
incoherent statements like "[1] is holding a surfboard". To minimize errors in modality translation,
we construct a critic model, trained on a limited set of high-quality, hand-annotated instances to
detect and remove such inconsistencies. This critic model mimics human judgment in evaluating the
generated commonsense knowledge, so that we can intentionally oversample localized knowledge
data, and utilize it to filter out non-relevant instances. Finally, we finetune a vision-language model
on the high-quality synthetic data to facilitate zero-shot localized visual commonsense reasoning.
We use 250K images in union of Visual Genome [26] and VCR [66], which include a diverse
set of social situations involving people and objects, as the seed images to collect the knowledge
corpus. After filtering, we collect 1M instances of Localized Commonsense Knowledge Corpus with
information grounded to specific regions in the image (see Appendix A for more details).
