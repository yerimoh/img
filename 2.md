Large transformers (Vaswani et al., 2017) pretrained with variants of a language modeling objective, such as BERT (Devlin et al., 2019), have
proven their effectiveness at flexibly transferring to
a variety of domains and tasks. One design decision that makes them particularly adaptable is their
graceful handling of the open vocabulary problem
through subword tokenization. Subword tokenization, popularized in the neural machine translation
literature (Sennrich et al., 2016; Vaswani et al.,
2017; Wu et al., 2016), produces tokens at multiple
levels of granularity, from individual characters to
full words. As a result, rare words are broken down
into a collection of subword units, bottoming out
in characters in the worst case.
