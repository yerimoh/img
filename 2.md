Next, we collect annotations of CQI data plausibility. Broadly, this follows West et al. (2022);
Howard et al. (2023) in combining generated data
with an automatic critic to maximize the quality
of a final trained model. In this case, however, we
explore multiple ways to incorporate annotations
of plausibility into the final model NOVACOMET
(§2.3.2).



Our primary means of collecting annotations of
plausibility is through Amazon Mechanical Turk.
We use a similar annotation template to (Hwang
et al., 2020) (see Appendix C), asking annotators to decide if knowledge is always/often, sometimes/likely, farfetched/never true, or invalid (giving these annotations a score of 3, 2, 1, 0 respectively). We consider any knowledge scored 3 or 2
to be plausible.



In practice, we collect 20k annotations, with 1 annotator per example. For underlying data, we use
16k examples from NOVATOMIC, as well as 2k examples each from ATOMIC10X and ATOMIC2020
to increase diversity of annotated knowledge style.
While these knowledge graphs have fixed relation
sets, we use sample natural language queries to
replace the discrete relations (e.g. xNeed → What
did PersonX need?).

We conduct a human agreement study on a segment of 200 examples for which we elicit 3 annotations each, finding Fleiss κ (Fleiss, 1971) of 0.317
indicating fair agreement (Landis and Koch, 1977).


