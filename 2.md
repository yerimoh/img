Symbolic knowledge distillation begins by going
machine–to–corpus, i.e. generating many commonsense facts, which results in a commonsense
knowledge graph. §2.1 frames this as sampling
to estimate the knowledge distillation objective–a
student commonsense model learns from the generations of a teacher (GPT-3).



We start with a loose teacher, transferring knowledge by prompted generation with truncated sampling alone–this is in contrast to the critical teacher
(§4) which explicitly judges and filters the generated samples. The loose teacher uses few-shot
prompting as in Brown et al. (2020). We use a
few-shot template:



where <EXi-INP>/<EXi-OUT> are humanauthored, natural language ATOMIC entries,
and <TASK-PROMPT> is a description of the
problem. Given such a prompt, GPT-3 generates
the missing piece, output <EXN -OUT> for input
<EXN -INP>, following the pattern of earlier
examples (1 to N-1). We find important aspects for
producing high-quality commonsense knowledge:



• Examples should be numbered. e.g.
<EX5-INP> might begin with "5)" to indicate it is the 5th example.

• The format of <EXi-INP> and <EXi-OUT>
should linguistically imply the relationship between them. See below for examples.

• <TASK-PROMPT> can be used to give extra
specification to complicated problems.
