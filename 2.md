Large language models (LLMs) such as GPT-4 have exhibited remarkable performance in reasoning
tasks (Rae et al., 2021; Lewkowycz et al., 2022; Zhong et al., 2023). Because of the intensive
computing resources required for training and hosting the LLMs for inference, many such LLMs
are only accessible via paid API services, thus leading to high monetary costs. In this work, we
are motivated to study strategies for reducing the costs of using LLMs while not sacrificing task
performance, particularly for LLMs’ applications to reasoning tasks.


Different types and versions of LLMs often come with different capabilities and costs. Typically,
LLMs with better performance (termed “stronger LLMs”) are more expensive than those with relatively worse overall performance (termed “weaker LLMs”). For example, GPT-4 (OpenAI, 2023) is
30 times more expensive than GPT-3.5-turbo for the output tokens.1
It thus implies a promising solution to cost-saving. That is, simple questions could be answered by the weaker but more affordable
LLM, whereas only the difficult questions need to be tackled by the more expensive, stronger LLM.
Drawing inspirations from here, Chen et al. (2023a) explored the idea of “LLM cascades”, where a
question is always first answered by a weaker LLM, and then optionally routed to a stronger LLM
when the the weaker LLM’s answer is not accepted (Figure 1). To decide this routing, this work
suggested fine-tuning a smaller LLM to score each question along with its answer produced by the
weaker LLM. While this approach could work for some tasks, in practice, we observed that it did
not yield satisfying performance for intricate reasoning tasks. Intuitively, it is very challenging to
evaluate the difficulty and the answer correctness of a reasoning question solely based on its literal
expression, even with a large enough LLM, since the errors could be nuanced despite the reasoning
paths appearing promising (Madaan et al., 2023).



In this work, we proposed to devise this routing decision-maker from a different angle, i.e., the
“answer consistency” of the weaker LLM (Wang et al., 2023). This is inspired by the observation
that answers from the weaker LLM tend to be consistent in multiple sampling paths when the question is easy, but inconsistent when the question is hard. To implement this idea, we proposed two
types of methods, a vote-based method that examines if the agreement of multiple answer samples
on the majority-voted answer surpasses a pre-defined confidence threshold, and a verification-based
method that checks if the majority-voted answers sampled from different prompts are consistent. To
realize the two methods in reasoning tasks, we further investigated multiple strategies for answer
sampling, including sampling from a single set versus two sets of task demonstrations. In particular, we proposed to leverage a “mixture of thought (MoT) representations”, which samples answers
from both Chain-of-Thought (Wei et al., 2022, CoT) and Program-of-Thought (Chen et al., 2022;
Gao et al., 2023, PoT) prompts, emulating how experts can provide diverse perspectives to the same
question. This follows the same spirit of ensembling (Rokach, 2010), but is applied to developing
LLM cascades for the first time. By pairing different sampling strategies with the two answer consistency checking methods (i.e., vote and verification), we end up with ten approaches to implementing
the LLM cascade.


To evaluate the proposed approaches, we conducted experiments on six reasoning datasets, covering
mathematical, symbolic, and causal reasoning tasks, using GPT-3.5-turbo as the weaker LLM and
GPT-4 as the stronger one. The experimental results demonstrated the effectiveness of LLM cascades based on answering consistency. That is, different approaches that we proposed for LLM cascades can generally achieve performance comparable to or even better than fully using the stronger
LLM, while they require only half or less relative cost to the latter. In particular, our approaches
based on a mixture of thought representations achieved comparable task performance with only 40%
of the cost of GPT-4. Our results also underscored the effectiveness of sampling answers from diverse prompt settings, such as sampling from different task demonstrations or different thought representations. Our further analysis revealed that different prompt settings can often provide different
opinions for the more complex questions while tending to be consistent for easier ones, which allows
us to distinguish questions at different difficulty levels more accurately for cascade decision-making.
Finally, we also compared our consistency-based approaches with fine-tuned smaller LLMs (Chen
et al., 2023a) as well as other variants that make the routing decisions based on the literal expressions
of the question and its answer. Our approaches exhibited strong advantages over all of them.
