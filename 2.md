Following our low-resource NER results, we evaluate entity representation quality even more directly
by measuring the zero-shot entity linking performance of both models. As shown in Table 10,
performance of our models exceeds the original
PubMedBERT results reported by Liu et al. (2021),
validating the quality of our pre-training procedure.
We note that the main difference between our pretraining setup and the original PubMedBERT setting is the use of the masked language modeling
(MLM) objective alone instead of both MLM and
next-sentence prediction (NSP) objectives. This
suggests that the use of the MLM objective only
might be better aligned with obtaining high quality
entity representations.



Additionally, when comparing our models, we
find that BioVocabBERT slightly underperforms
the PubMedBERT replica on all datasets except the
more diverse MedMentions dataset. However, we
note that the improvements obtained in the MedMentions dataset are also quite small at under 1%.
Given the zero-shot nature of this experiment, it
suggests that the entity representations obtained by
these two models are of comparable quality and
that PubMedBERT’s pre-training enables a high
degree of robustness around sub-optimal tokenization.


5.4 Case Study: Tokenization Robustness

As shown in the NER and entity linking experiments above, the downstream performance of
biomedical language models appears to be mostly
robust to biomedical concept segmentation which
is not well-aligned with human judgments. To analyze this phenomenon, we take a closer look at
how our pre-trained PubMedBERT model represents biomedical concepts which are segmented in
apparently erroneous ways by the PubMedBERT
tokenizer. Table 9 contains two words from the
biomedical subset of SIGMORPHON which were
sub-optimally segmented by PubMedBERT. For
each of these words we include two sets of their
5 nearest neighbors according to different embedding types. The first set shows the nearest neighbors obtained using embeddings computed by averaging all subword embeddings that make up a
specific word. The second set comes from using
the ‘[CLS]’ token embedding of our PubMedBERT
model, often used for downstream tasks in standard
fine-tuning. The pool of words from which these
neighbors are obtained consists of all the unique
UMLS words in UMLS phrases used in the construction of BioVocabBERT


Since it comes directly from subword embeddings, the first set of neighbors is meant to show
whether the tokenizer’s sub-optimal segmentation
introduces a bias which distracts the model from
the true semantics of a biomedical term. The second neighborhood is meant to more faithfully show
us how the model represents a biomedical concept.
Comparing these two sets can let us determine if
the bias introduced by subword embeddings is successfully regulated by the overall model.



We first observe that the bias we expected to
find in the word embedding neighborhoods is evidently present. Most words in these first sets are
segmented in exactly the same ways as the original
sub-optimally segmented word. As seen in Table
9, the word ‘neuromodulation’ is segmented by
PubMedBERT as ‘neuromod-ulation’, splitting the
meaningful ’modulate’ morpheme down the middle, an example of the compound error in Table 4.
Due to this, other words with the same subword but
different semantics such as ‘immunomodulation’
(‘immunomod-ulation’) and ‘immunoregulation’
(‘immonoreg-ulation’) are added to the word embedding neighborhood. This is also seen in the second example, where the word embedding neighbors
of ‘epicarditis’ (‘epic-ardi-tis’) all contain at least
two of the three original subwords. If these word
embeddings were the final model representations,
this bias could lead to considerable errors in downstream tasks like entity linking by up-weighting
terms based on sub-optimal subwords.





Fortunately, we observe that the language model
is able to readily overcome the bias observed in
the word embeddings when it comes to the final ‘[CLS]’ representations. The second neighborhoods often contain semantically relevant words
which were segmented differently than the original,
such as ‘neuroexcitation’ (‘neuro-exc-itation’) and
‘neuroregulation’ (‘neuro-reg-ulation’) for ‘neuromodulation’ (‘neuromod-ulation’) or ‘perimyocarditis’ (‘peri-my-ocardi-tis’) for ‘epicarditis’
(‘epic-ardi-tis’), which both mean types of inflammation of the pericardium. This shows us that the
language model successfully extracts the semantics of the morpheme ‘neuro’ from ‘neuromod’ as
well as the cardiovascular related semantics from
both ‘epic-ardi’ and ‘ocardi’, effectively mitigating
the detrimental effects seen in the word embedding
neighborhoods from sub-optimal tokenization. We
thus conclude that this same robustness is responsible for the parity observed in downstream tasks
between BioVocabBERT and the original PubMedBERT. More examples which show similar trends
as the ones in Table 9 can be found in Table 14 in
Appendix C.

















