LLMs in the Cascade: This includes GPT-3.5 Turbo
and Llama 2 70B Chat with zero-shot task prompting
(details in Appendix B.2). The LLM outputs are also
used as the annotations for online cascade learning and
distillation.
• Knowledge Distillation: We fine-tune smaller models
using different portions of LLM annotations. To ensure
fairness, datasets are split equally, with 50% prepared
for training (as distillation labels) and the remaining 50%
for testing. All methods are evaluated on the identical
test sets. In our experiments, the distilled smaller models
are used in isolation without any ensemble or cascade.
• Online Ensemble Learning: We employ all available
models in an ensemble with learned predetermined probabilities. The smaller models are also continuously updated based on LLM annotations. This serves as an
ablation of our method by excluding the deferral policy
learning component.
