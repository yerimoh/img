Large language models are capable of efficiently performing a wide array of tasks in a zero-shot
fashion. For text-only models, one commonly adopted interface is a flexible, language specification of inputs coupled with an imperative request, e.g., “[article text]. Summarize this
article." Similarly, a natural extension allowing visual inputs manifests as, e.g., “[image].
Describe this image".




But, as models expand beyond text-only modalities, they should incorporate more flexible forms of
user input as well. Allowing users to specify individual objects/actors/regions within an image as part
of the input query is an important challenge, e.g., the [image] [request] interface above would
not directly a user to ask Why is [this person in the image] sad?. One option would to
simply require users specifically describe the piece of the image they are attempting to specify,
e.g., “[image] [description of specific region] [request]". But authoring concrete
referring expressions is not only cumbersome, particularly for scenes with lots of objects (e.g., “the
person in the red jacket on the left of the scene with their arms crossed") but also challenging, even for
humans: [11] argue that a good referring expression should both specify the reference precisely, but
also, follow Grice’s maxim of Quantity, i.e., provide no extra information. Given this tension, many
popular referring expression datasets are gathered in a sophisticated “gamified" fashion [53, 22],
which aims to balance underspecification vs. verbosity.



We argue instead that users of vision-augmented LLMs should instead be able to pass localized visual
references simply by “pointing" to regions within the image [4, 48, 40]. This enables models to
focus on the region while interpreting the user’s request in a more intuitive fashion, and provide
more accurate and contextually relevant responses. By incorporating localized visual references, the
model can better understand and interpret complex scenes, thereby improving its performance on
tasks requiring a detailed understanding of the visual context.





We propose Localized Symbolic Knowledge Distillation (LSKD): the core idea is to provide literal
descriptions of images to a large language model, and allow that model to connect-the-dots between
these literal descriptors (e.g., lists of objects) and a holistic perspective of the scene. Different
from recent works which also distill from an LLM conditioned on visual descriptors symbolically
[34, 74], we additionally provide a localized reference to a particular region within the image and
design prompts to encourage the LLM to generate commonsense inference about that specific region.
After sampling, we train Localized Visual Commonsense models to generate commonsense triples
conditioned on the image and the region directly; we show that this process effectively distills the
LLM’s capacity for global+local scene understanding highlighted by zero-shot results on localized
visual reasoning benchmarks and human evaluation.



In summary, our main contributions are

1. A new scalable framework that can generate reliable and localized visual commonsense statements.
  
3. The Localized Commonsense Knowledge Corpus: 1M localized commonsense inferences posed
over 250K images. This dataset can be used to expand the capacity of existing vision+language
models to incorporate references-as-input with no architectural modifications.


5. Achieving the SoTA zero-shot performance for three localized visual reasoning tasks.

   
7. Human evaluation results suggesting that a strong student model outperforms the teacher model in
answering localized visual commonsense questions

















