Large language models (LLMs) (Bommasani et al., 2021;
Touvron et al., 2023b; Brown et al., 2020) hold great promise
as a means of one-pass query answering over text streams.
For example, suppose we have a stream of movie reviews
posted on the internet and would like to retrieve the ones
that are positive (Figure 1). To address this without human
annotations, we can create a prompt for the LLM to identify
the sentiment in a review. The LLM can then use this prompt
to process each statement in sequence.



However, LLM inference can be extremely expensive, especially in a streaming setup where queries arrive continuously. Based on our benchmarking (Appendix B.1), a GPU
server with eight A100 GPUs takes 3.6 seconds to process a
document containing 8,192 tokens using the largest Llama
(Touvron et al., 2023a). To process one million such documents per hour would thus require 1,000 A100 servers.
Using Amazon Web Services, this computation would cost
more than 30,000 USD per hour, if it were even possible to
procure the machines required.


There are two popular ways to reduce the cost of LLM
inference. The first is to distill a large language model into
a smaller model that can process a document using less
computation (Hinton et al., 2015; Gu et al., 2023; Hsieh
et al., 2023). The second is to use a cascade of models that
use smaller models to process “easier” inputs and reserve
the largest models for the most difficult inputs (Varshney
& Baral, 2022; Chen et al., 2023). However, these existing
proposals assume a model of learning in which a labeled
training set is available beforehand, making them unsuitable
for a streaming setting. In contrast, the streaming setting
requires online learning (Hoi et al., 2021).

In this paper, we propose online cascade learning to bridge
this gap in the literature. Our goal here is to develop a cascade of models, arrayed from the least to the most complex,
that can process queries in a data stream with optimal costperformance trade-offs. The critical difference from prior
work is that our cascades are trained in a fully online manner
and do not require any human-labeled training data. The
smaller models in the cascade would continuously evolve
and improve over time by imitating the LLM’s demonstrations on the harder queries, enabling them to handle an
expanding range of queries with increasing proficiency.


A key component of our cascades is the deferral policy
that decides, given an input, the best “level” of the cascade
that should handle the input (Figure 2). At startup, the
policy keeps its “gates” open, allowing all initial inputs
to flow through the cascade and be processed by the most
expensive model (an LLM). These processed inputs then
become training labels for updating the smaller models and
the deferral policy within the cascade. Over time, as the
model sees more data, the system stabilizes at a state where
the smaller, less expensive models can handle the majority
of the new inputs. The framework also incorporates a set of
learning hyperparameters that adjust the trade-off between
accuracy and cost based on user needs.


We formalize the problem of online cascade learning in
terms of an episodic Markov decision process (MDP) that
considers both prediction loss and computational costs for
co-optimization. We assume an expert policy — a highcapacity LLM — for this MDP. We learn the various components of the cascade through imitation learning (Ross et al.,
2011) based on the LLM demonstrations. We show that
our algorithm comes with a theoretical no-regret guarantee.
Our experimental results, on four tasks of various complexity, show that our proposed method can achieve accuracy
comparable to LLM at a vastly reduced inference cost.



To summarize the main contributions of our work:

• We introduce online cascade learning, a new framework
for learning model cascades in resource-intensive streaming analytics settings. The framework enables systematic
trade-offs between prediction accuracy and resource usage, and allows learning without any human annotations.


• We offer a formulation of the online learning of cascades
in terms of episodic MDPs and give a no-regret imitation
learning method for solving this problem.



• We present rigorous experiments showing that our proposed algorithm can achieve comparable accuracy as
LLMs while saving up to 90% of the inference costs. Our
source code is available at https://github.com/
flitternie/online_cascade_learning.




















