3.3 ANALYSIS ON MIXTURE OF THOUGHT REPRESENTATIONS



To understand the effect of MoT, we analyze three vote-based approaches. We first group questions
into “easy” and “hard” based on whether the weaker LLM can answer them correctly (i.e., whether
the majority-voted answer Aw is correct or not). For all easy/hard questions, we then calculate
the average consistency score following Eq 2. We present the average consistency scores for each
approach, categorizing easy and hard questions based on whether the weaker LLM can answer them
correctly (i.e., Aw being correct). A higher average consistency score indicates our decision-maker
places greater trust in the answer from the weaker LLM. Hence, an effective cascade decision
maker should reveal relatively higher consistency scores for easy questions and lower ones for hard
questions, leading to a larger ”gap” between them.








We show the results in Figure 4 (Top). It is observed that all approaches lead to higher consistency
scores on the easy questions than those on the hard questions, which explains their overall effectiveness for cascade decision-making. Because of involving two different thought representations,
MoT-1D-Vote tends to have a lower consistency score compared with the two CoT approaches.
However, it still ends up with a larger “gap” in the consistency scores for the easy and the hard
questions, which is particularly prominent on the Navigate dataset where it gives the best performance gain (Figure 3). In contrast, CoT-1D-Vote results in the smallest score gap, indicating
its weakness in distinguishing between the easy and the hard questions, particularly for Navigate.
This weakness is mitigated by diversifying the prompting with two sets of task demonstrations (i.e.,
CoT-2D-Vote), but it still underperforms mixing the thought representations.




Finally, in Figure 4 (Bottom) we show that when CoT cannot answer a hard question, prompting the
weaker LLM with another set of task demonstrations (but still under the CoT representation) often
yields the same mistaken answer, which thus results in a high consistency score. On the contrary,
PoT tends to make mistakes in a very different way and result in a different mistaken answer, which
explains the low consistency score of MoT-1D-Vote. More cases are shown in Appendix D.





3.4 ROBUSTNESS EVALUATION


We further analyze if our results are sensitive to the change of the sampling temperature T or the
sample size K. We select datasets for each type of reasoning task and conduct experiments with
CoT-2D-Vote and MoT-1D-Vote. Our results are shown in Figure 5. We first look into the
effect when increasing the sampling temperature T from 0.4 (our default setting) to 0.8. For both
approaches, increasing their temperature yields comparable or slightly better performance. This is
owing to the increased answer diversity when the temperature gets higher. However, in any case,
MoT-1D-Vote consistently outperforms CoT-2D-Vote. Increasing the sample size K from 20
to 40, on the other hand, leads to a rightward shift of the curves, implying that it requires higher cost
to achieve the same task accuracy. This can be explained by the higher cost of the larger sample size,
whereas increasing the sample size does not contribute to the detection of easy vs. hard questions.
Like in the results of varying temperatures, MoT robustly outperforms CoT in any case.







3.5 COMPARISON TO EXTERNAL TEXT-BASED VERIFIERS






As mentioned, prior work implemented the LLM cascade decision-maker by training an external
verifier, which scores a question and its answer (from the weaker LLM) based on their literal descriptions (Chen et al., 2023a; Sakota et al., 2023). Related to the above work, Chen et al. (2023b) ˇ
and Madaan et al. (2023) also showed the promise of prompting LLMs to evaluate their own responses. To perform a general comparison with such external verifiers, we conducted experiments on
GSM8k, DATE, and CREPE with the following baselines: Finetuned-Q, which is a RoBERTabase model (Liu et al., 2020) fine-tuned to decide whether a question should be routed to the stronger
LLM based on its description; Finetuned-QA, which works similarly as Finetuned-Q but additionally takes the majority-voted answer from the weaker LLM (GPT-3.5-CoT-SC) as input;
LLM-Q, where we instead prompt GPT-3.5-turbo as the verifier to judge based on the question description; and LLM-QA, which similarly employs GPT-3.5-turbo to decide upon the question and
the weaker LLM’s majority-voted answer. We leave details of the baselines in Appendix E.





The results in Figure 6 show that incorporating the external verifiers cannot achieve comparable
accuracy with GPT-4-CoT-SC. For example, on the GSM8k dataset, the highest accuracy with the
external verifiers is 0.892, which is way lower than the accuracy (0.958) of GPT-4-CoT-SC and
the accuracy (0.951) of our approaches. They also show lower task accuracies than our approaches
under the same cost. It indicates that the external verifiers cannot yield satisfying results in complex
reasoning tasks, which can be due to the intrinsic challenge of deciding question difficulty and
answer correctness solely based on their textual descriptions. For the calibration analysis, please
refer to the appendix I






3.6 ADDITIONAL STUDIES


How weak can the weaker LLM be? 




We evaluate our approaches when adopting LLAMA2-13B
(Touvron et al., 2023) as the weaker LLM and GPT-4 as the stronger LLM. The results are shown
in the Appendix F. On the DATE dataset, our approaches still works. However, LLAMA2-13B
as the weaker LLM doesn’t yield ideal results on GSM8k and CREPE. That is because most of the
questions in GSM8k and CREPE are excessively complex for LLAMA2-13B. Therefore, LLAMA2-
13B often fails to answer the questions consistently across multiple samplings. Hence, the choice of
a weaker LLM should be contingent on the task’s level of difficulty. When the current weaker LLM
struggles with the task, it is advisable to consider switching to a more powerful LLM.





Can the stronger LLM benefit from the weaker LLM hints? In our LLM cascade, all questions
will obtain answers from the weaker LLM, no matter if they will be sent to the stronger LLM
or not. Therefore, an interesting question is whether the answer produced by the weaker LLM
(correct or incorrect) can provide “hints” to enhance the stronger LLM. To answer this question, we
experimented with MoT-1D-Verify, where we additionally passed the two inconsistent answers
Aw′
1
and Aw′
2
as hints to the stronger LLM following the format of prior work (Zheng et al., 2023).
We observed that the hints can only yield slight improvement on DATE, but greatly hurt the model
performance on GSM8k and CREPE. Therefore, we conclude that hints from the weaker LLM,
when it is uncertain about the question, do not help the stronger LLM. Details are in Appendix G.





Can our method generalize to factual-based tasks? 


We also explored whether our method can
be generalized to factual-based reasoning tasks in Appendix J.







