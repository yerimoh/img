To learn cascades online without human annotations, we
propose an imitation learning algorithm that assumes an
expert policy (an LLM) that can demonstrate ground-truth
labels. The algorithm iteratively updates the classification
models and deferral functions in the cascade by imitating
the expert as in DAgger (Ross et al., 2011). However, unlike
traditional imitation learning, our goal here is to balance
computational efficiency and accuracy.


We incorporate such an expert into our cascades by assuming that the final classification model mN is the expert LLM.
When invoked on a query xt, mN always outputs a vector
whose largest value is associated with ground-truth label
yt. However, it may not always be the case that choosing to
invoke mN leads to optimal cost. There may be a smaller
model mi for i < N that also produces the correct label yt,
without the cost of the additional defer actions. Indeed,
an optimal policy may occasionally incur prediction errors
to manage the cost of incurring defer actions. Designing
an algorithm that can train a policy in an online fashion to
manage those trade-offs is at the core of the paper.



Our overall algorithm is detailed in Algorithm 1. Here,
m1, ..., mN are the classification models (with mN being
the expert for imitation) and f1, ..., fN−1 are the deferral
functions. The smaller models m1, ..., mN−1 are initialized
either randomly or with their respective pretrained weights.
For each incoming query xt (outer loop), we sequentially utilize the i-th model in the cascade to make a prediction (inner loop). Specifically, the model generates a probability vector
predi
, which is processed by the deferral function fi
. This
yields a deferral probability score that can be discretized
into a binary actioni
to determine whether the prediction at
the current cascade level is reliable.




If the action is defer, the query would be navigated to the
cascade’s next (i + 1)-th model. Otherwise, it will make a
prediction yˆt = arg max(predi
), and break the inner loop
(succeeding models will not be activated for current query
xt). If the query has been deferred to the LLM expert mN
at the last cascade level, its annotation yˆt is regarded as the
ground truth yt and aggregated to the dataset D.



Throughout the inner iteration, at cascade level i, it may
optionally skip the rest layers and jump to the LLM expert
mN at a non-zero decaying probability βi
to directly obtain
its demonstration and aggregate it to dataset D, similar to
DAgger (Ross et al., 2011), for faster convergence.


After processing each query, the algorithm outputs the prediction yt, then updates the small models m1, ..., mN−1 to
mimic the expert demonstrations on the collected trajectories D. Similarly, the deferral functions f1, ..., fN−1 are
also updated based on the loss computed by Equation (1).
The algorithm continuously collects annotations from the
LLM expert (e.g., at a decaying probability βt or when the
query is deferred to mN ) and updates the policy via online
gradient descent (OGD). Practically, the user can change
the cost weighting factor µ in the loss function J(π) and the
initial decaying factor β1 for adjusting cost budgets.
