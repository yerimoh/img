We instantiate the proposed two methods in LLM reasoning tasks with different sampling sources,
resulting in 10 approaches, as summarized in Figure 2. Specifically, 6 approaches adopt vote-based
decision-making: CoT-1D-Vote collects K answers sampled from prompting the weaker LLM
with the CoT representation, and then calculates the answer consistency for decision-making, following Eq 2. Similarly, PoT-1D-Vote bases its decision-making on answers sampled from a
PoT prompt. To diversify the sources of the answers, for each thought representation, we further
consider sampling from two sets of CoT or PoT demonstrations, resulting in CoT-2D-Vote and
PoT-2D-Vote, respectively. Finally, the vote-based approaches also include two variants leveraging a mixture of thought (MoT) representations. For MoT-1D-Vote, K1 answers are sampled
from the CoT prompt and another K2 from the PoT prompt, and a union set of their answers are
then used to compute the consistency score s. For MoT-2D-Vote, the procedure is similar, except
that the CoT and the PoT prompts are annotated from two sets of demonstration examples.


The verification-based approaches assume answer samples from two different prompts. We
instantiate 4 variants, including CoT-2D-Verify, where we prompt the weaker LLM with
two sets of CoT demonstrations, resulting in two answer sets for decision-making (Eq 3);
PoT-2D-Verify, where we similarly prompt the weaker LLM with two sets of PoT demonstrations; MoT-1D-Verify, where we consider two sets of answers from two thought representation
prompts (but on the same set of task demonstration examples); and MoT-2D-Verify, which additionally employs different sets of demonstrations when prompting the weaker LLM with different
thought representations.



Cost-Comparable Sample Size Configuration 

To fairly compare all approaches in terms of their
effectiveness in identifying easy (or correct) vs. hard (or correct) questions, we aim to configure
their costs to be comparable. Since we use the same prompt to the stronger LLM, C
s
is agnostic
to the specific approach. However, different approaches may require varying costs in calling the
weaker LLM (i.e., C
w). For example, even with the same total sampling size (i.e., K = K1 +
K2), CoT-1D-Vote and CoT-2D-Vote result in different input token usages because the former
prompt the LLM once but the latter prompt it twice, doubling the token usages. Therefore, we unify
the C
w by a coarse-grained cost alignment. Our analysis suggests different configurations of K for
different approaches. We refer readers to Appendix A for more details. Since our configuration can
only yield comparable costs due to certain simplifications, rather than exact costs, we still report the
actual token cost for each approach.
