In order to discover whether morpheme segmentation performance has an effect on the biomedical
language model pre-training process, we compare
the downstream performance of two language models using tokenizers with very distinct segmentation
performance but otherwise equivalent pre-training
processes. The first model is pre-trained using the
same tokenizer as PubMedBERT while the other
one uses our designed BioVocabBERT tokenizer
and is thus referred to by the same name.



As other BERT based architecture models, we
pre-train these using the masked language modeling objective and choose standard token masking percentages used in previous work (Gu et al.,
2021). We use the easily accessible and readily preprocessed corpus used for BlueBERT (Peng et al.,
2020) pre-training which contains around 4 billion
words3
. For pre-training, we base our implementation on the work by Izsak et al. (2021) to obtain the
most efficient pre-training possible. We describe
the data, optimization steps, batch size, hardware
and other pre-training details used for both models
in Table 13 and Appendix B.



4.2 Tasks

In order to determine how tokenization improvements affect the quality of biomedical concept
representation in language models, we narrow our
task selection to those which are more closely
related to entity understanding instead of overall
sentence understanding as is the case with relation
extraction, sentence similarity or natural language
inference. We select named entity recognition
(NER) and entity linking (EL), also referred to as
concept normalization, as the two biomedical NLP
tasks which most closely meet this criterion.


NER.

We focus on evaluating our models in the
more standard fully supervised fine-tuning NER
setting as is done in previous work (Lee et al., 2019;
Gu et al., 2021). We run hyperparameter tuning on
the development set, the search space used can be
found in Appendix A.



We also study our models’ low-resource NER
performance using 500 and 1000 examples. We
also carry out hyperparameter selection which can
be found in Appendix A. We report results on the
development set only for our low-resource NER
experiments.




Entity Linking. 



For entity linking, we evaluate
our models’ zero-shot performance as done by Liu
et al. (2021) which allows us to measure entity
representation quality as directly as possible.








