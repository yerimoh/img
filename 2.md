2.1.1 Vocabulary. We assume that the input consists of text spans, such as sentences separated by special
tokens [SEP]. To address the problem of out-of-vocabulary words, neural language models generate a vocabulary
from subword units, using Byte-Pair Encoding (BPE) [51] or variants such as WordPiece [32]. Essentially, the
BPE algorithm tries to greedily identify a small set of subwords that can compactly form all words in the given
corpus. It does this by first shattering all words in the corpus and initializing the vocabulary with characters and
delimiters. It then iteratively augments the vocabulary with a new subword that is most frequent in the corpus
and can be formed by concatenating two existing subwords, until the vocabulary reaches the pre-specified size
(e.g., 30,000 in standard BERT models or 50,000 in RoBERTa [39]). In this paper, we use the WordPiece algorithm
which is a BPE variant that uses likelihood based on the unigram language model rather than frequency in
choosing which subwords to concatenate. The text corpus and vocabulary may preserve the case (cased) or
convert all characters to lower case (uncased).
