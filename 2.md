The “de facto” BERT model was pre-trained on BooksCorpus (800M words) (Zhu et al., 2015) and
English Wikipedia (2.5B words) (Devlin et al., 2018). The BioBERT model from (Lee et al., 2019)
was pre-trained on different combinations of text-data from English Wikipedia, BookCorpus, PubMed
Abstracts (4.5 billion words) and PubMed Central full-text abstracts (13.5 billion words). We trained the
BioMedBERT model on BREATHE containing over 6 million articles from 9 different archives with 4
billion words. In pre-processing our dataset, we treated lists, tables and headers as a contiguous sequence
of text and initialized our model using the pre-trained BERT weights. Initially we trained the model from
scratch with a custom SentencePiece vocabulary for tokenization, but this did not yield good results
when evaluated on downstream fine-tuned tasks. We represented the input sequences as vectors using
the WordPiece embeddings, which contains 30,000 tokens (Wu et al., 2016). Using WordPiece allowed
us to better leverage the pre-trained weights of BERT. WordPiece has a clever formulation to account for words not found in its vocabulary by breaking a word into subwords, thereby creating multiple tokens
for a given word. Also, WordPiece has a subword for every character in the alphabet.
