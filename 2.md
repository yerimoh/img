We evaluate our LLM cascade approaches on six datasets, covering (1) mathematical reasoning,
including GSM8k (Cobbe et al., 2021), ASDIV (Ling et al., 2017), and TabMWP (Lu et al., 2023);
(2) symbolic reasoning from BIG-Bench Hard (bench authors, 2023), including DATE and Navigate; and (3) causal reasoning, including CREPE (Zhang et al., 2023). In our pipeline, we leverage
the GPT-3.5-turbo (4k context) as the weaker LLM and the GPT-4 (8k context) with CoT selfconsistency (Wang et al., 2023, SC) as the stronger LLM. Throughout our experiments, we set the
number of task demonstrations as M = 8. We use the same demonstration examples as prior
work (Chen et al., 2022; Wei et al., 2022). When additional task demonstrations are needed (e.g.,
for 2D approaches), we randomly sample examples from the training data and manually annotate
them with thought representations. We set the number of sampling paths as K = 20 for GPT-3.5-
turbo and K = 3 for GPT-4. The sampling temperature by default is 0.4 for both LLMs. The
metrics we use are the task accuracy and the relative cost compared with the cost of GPT-4 with
CoT SC (denoted as GPT-4-CoT-SC). A lower relative cost and higher accuracy indicate better performance. We also compared our approaches with baselines using only the weaker LLM
(i.e., GPT-3.5-CoT-SC, GPT-3.5-PoT-SC) or only the strong LLM in different ways (i.e.,
GPT-4-CoT-Greedy, GPT-4-PoT-Greedy, GPT-4-CoT-SC, GPT-4-PoT-SC). Our experimental details can be found in Appendix B and reproducible prompts in Appendix L.
