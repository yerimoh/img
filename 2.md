We leverage a cascade of LLMs to save the cost
of in-context LLM reasoning, as illustrated in
Figure 1. Specifically, we assume two LLMs.
The weaker LLM (denoted as LLMw) yields
relatively worse performance but is less costly,
whereas the stronger LLM (denoted as LLMs
)
enjoys better task performance but is more expensive. Given a question Q, the LLM cascade
first employs the weaker LLM to obtain an initial answer Aw. This answer, along with other
metadata produced by the weaker LLM, will then be fed to a cascade decision maker to decide
whether the answer can be accepted as the final one. If the answer is rejected, the stronger LLM
should be invoked to provide a more reliable answer As
. As a consequence, the total cost of answering the question becomes
where C
w and C
s
indicate the costs from calling the weaker and the stronger LLMs, respectively,
C
d denotes any cost involved in the LLM cascade decision-making process, and 1reject = 1 holds
if and only if the decision maker rejects the answer.



Both LLMs solve the question via few-shot in-context learning, e.g., for the weaker LLM, an answer
Aw is produced by sampling from PLLMw (Aw |E1||E2|| . . . ||EM||Q), where E1||E2|| . . . ||EM||Q
denotes a concatenation of M task demonstrations and the input question Q, forming the “prompt
input” to the LLM (Brown et al., 2020). As M task examples are used to demonstrate the task,
it indicates a “M-shot in-context learning” of LLM. For reasoning tasks, in practice, a LLM will
typically be prompted to elaborate on its reasoning process via “thought representations”, such as
Chain-of-Thought (Wei et al., 2022, CoT) and Program-of-Thought (Chen et al., 2022; Gao et al.,
2023, PoT), where the reasoning process is described step by step via natural language and programming language, respectively. The answer (e.g., a numerical result of the mathematical calculation)
can then be extracted from the texts (for CoT) or obtained by executing the code (for PoT).
