As opposed to general English, many concepts
in biomedical terminology have been designed
in recent history by biomedical professionals
with the goal of being precise and concise.
This is often achieved by concatenating meaningful biomedical morphemes to create new
semantic units. Nevertheless, most modern
biomedical language models (LMs) are pretrained using standard domain-specific tokenizers derived from large scale biomedical corpus statistics without explicitly leveraging the
agglutinating nature of biomedical language.
In this work, we first find that standard opendomain and biomedical tokenizers are largely
unable to segment biomedical terms into meaningful components. Therefore, we hypothesize
that using a tokenizer which segments biomedical terminology more accurately would enable
biomedical LMs to improve their performance
on downstream biomedical NLP tasks, especially ones which involve biomedical terms directly such as named entity recognition (NER)
and entity linking. Surprisingly, we find that
pre-training a biomedical LM using a more accurate biomedical tokenizer does not improve
the entity representation quality of a language
model as measured by several intrinsic and extrinsic measures such as masked language modeling prediction (MLM) accuracy as well as
NER and entity linking performance. These
quantitative findings, along with a case study
which explores entity representation quality
more directly, suggest that the biomedical pretraining process is quite robust to instances of
sub-optimal tokenization.
